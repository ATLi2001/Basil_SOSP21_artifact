\section{On the quest for scalable, decentralized, fault tolerance}
\fs{some intro sentence.} 
In the following we discuss both the limitations of existing solutions and desirable properties that drive Indicus' design. 

\subsection{In light of partial ordering}
Imposing a shared total order across different machines solves the problem of State Machine Replication, by a reduction of problem, rather than solving the original goal: providing consistent output. While execution in a common order implies agreement on a common result, this additional step adds coordination overhead, when possibly unecessary, as each transaction is declared to be co-dependent with all other transactions. 
Concretely, when requiring agreement on a totally ordered ledger of transactions, achieving agreement on each new request implies achieving agreement on the entire history prefix. Many large-scale distributed systems consist primarliy of unordered and unrelated operations. For example, a product supply chain consists of many concurrent steps that do not require ordering and can reach agreement in parallel.  Thus, in practice, where only a subset of transactions conflict, a partial order suffices. 
Existing blockchain technologies \fs{such as Hyperledger Fabric} often consider themselves shared databases, yet fall back to restrictive total order broadcast primitives, hence fundamentally limiting scalability. 


\subsection{A shard of truth}
To combat this scalability bottleneck many systems rely on sharding. While sharding approaches in practice \fs{(often)} can induce partial ordering and increase parallelism, this is not a principled property as the application of sharding conflates two objectives:
At its core, sharding horizontally scales hardware resources, reducing memory overheads such as storage cost or lookup time \fs{, and general CPU load, such as messages and agreement instances to manage}. In order to maintain a consistenct database, sharding requires coordination across shards, typically in the form of a joint two-phase commit and concurrency control protocol. When transactions span multiple shards, the latent total order requirment within each shard introduces redundant coordination overheads, as coordination may need to be performed twice, at the level of individual shards, and across shards. This is especially problematic when workloads are geo-replicated, or when, as in BFT, the replication factor is high. 
We argue, that there exist no optimal calibration for the degree of sharding: When sharding is coarse (few large shards), scalabiliy suffers under the per-shard total order, wheras when sharding is fine-grained (a lot of small shards), cross-shard overheads begin to dominate. 
Note moreover, that chosing a suitable partition data mapping is highly workload dependent and hence \fs{from a system design perspective} is not agnostic to the application.

Zhang et al \cite{zhang2016operation, zhang2015tapir} were the first to point out that by integrating both replication and transaction layer, and consequently relaxing the total order requirement within shards, redundant coordination overheads can be reconciled.
In Indicus we adopt this rationale by co-designing concurrency control and replication, thus naturally exposing existing commutativty between transactions.



\subsection{Decentralizing decentralized systems}
Blockchain technologies are often advertised as a shared databases that decentralize trust (i.e. Hyperledger). In order to to do so, permissioned Blockchains rely on traditional BFT SMR, such as the seminal PBFT protocol \cite{castro1999practical} and its descendants \cite{castro1999practical, kotla2007zyzzyva,  gueta2018sbft, clement2009making, buchman2016tendermint, yin2019hotstuff}, that provide a totally ordered log abstraction.
In practice, most of these solutions intertwine the requirement for a total order with the existance of a dedicated leader replica that acts as centralized sequencer, thus exposing two obvious concerns: scalability and fairness.

A single sequencer exposes a proposal bottleneck, as load is focused entirely on the leader while other replicas sit idle. Moreover, when the leader is faulty, i.e. crashes or misbehaves, the system comes to a halt, requiring expensive re-election schemes to re-gain liveness while maintaining consistency. 

Moreover, a dedicated sequencer exposes a fairness vulnerability. A malicious, or simply rationally biased proposer may censor, delay or frontrun transactions with only limited accountability. Such authority is antithetical to the ethos of decentralized trust and thus highly undesirable. 

These restrictions motivate us to design a leaderless protocol. Concretely, we avoid proposal delegation to an untrusted replica by declaring each client to be its own transaction coordinator.
By shifting responsibility from replicas to clients, the proposal bottleneck becomes the max-capacity of incoming requests replicas can process, while fairness is entirely up to the network delivey. In doing so, the crux of the design becomes how to tolerate byzantine client behavior. In particular, byzantine participants should not be able to interfere with honest participants in an unbounded manner and without being held accountable for their participation. \\


In this paper we show how to complete this Quest, resulting in our design and implementation of Indicus. In the following we define our system model as well as general properties for reasoning about a byzantine database.