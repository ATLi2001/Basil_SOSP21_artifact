
%\input{sections/Indicus/Protocol/subsections/Execution.tex}
%\input{sections/Indicus/Protocol/subsections/Validation.tex}
%\input{sections/Indicus/Protocol/subsections/ConcurrencyControl.tex}
%\input{sections/Indicus/Protocol/subsections/ConsistentLogging.tex}
%\input{sections/Indicus/Protocol/subsections/Writeback.tex}
%\input{sections/Indicus/Protocol/subsections/MultiSharding.tex}
%\input{sections/Indicus/Protocol/subsections/Failures.tex}
%\input{sections/Indicus/Protocol/subsections/Variations.tex}

%-------------------------------------------------------------------------------
\section{Protocol}
%-------------------------------------------------------------------------------
Indicus comes in two different flavors, Indicus3 and Indicus5, relying on varying replication degrees. Indicus3 requires 3f+1 replicas per shard to guarantee consistency, the minimum bound necessary for BFT SMR. Indicus5 uses a higher replication degree of 5f+1 replicas, but in return brings down both gracious execution latency and complexity during uncivil executions. We argue, that unlike past system settings where a single authority strives to maintain the fewest amount of replicas necessary for cost considerations, consortium systems with naturally higher replication degree are willing to pay the additional price for performance. 
We remark, that both Indicus3 and Indicus5 follow the same ethos and implement the same architecture, with minor differences in detailed protocol realizations. For the simplicity of exposition we discuss Indicus5 for the remainder of the paper. In section X we briefly describe the differences in Indicus3.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Figs
\begin{figure}[t]
  \begin{mdframed}[roundcorner=10pt]
 	\textbf{TX Exec State}
 	\begin{itemize}
 	\item ClientID
 	\item ClientSeqNo
 	\item InvolvedShards = \{S\}
 	\item ReadSet = \{(key, version)\}
 	\item WriteSet = \{(key, value)\}
 	\item dependencies = \{$\langle (key, version, \{TxID'\})_{f+1 \sigma} \rangle$\}
 	\item TxID = H(TX)
 	\item Timestamp = (Time, ClientID)  optional:, TxID) this is just deterministic tie breaker when a client misbehaves.
 	\end{itemize}
  \end{mdframed}
  \caption{Transaction Execution state}
  \label{fig:TX}
\end{figure}

\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[-] #1}
}

\begin{figure}[t]
  \begin{mdframed}[roundcorner=10pt]
 	\textbf{Replica }
 	\begin{itemize}
 	\item ReplicaID
 	\item ShardNo
 	\item LocalClock
 	\item RTS = \{(key, \{(TS, \{dep\})\})\}
 	\item Ongoing:
 	\subitem OngoingTX = \{(TxID, [TX, val state])\}
 	\subitem PreparedDB = \{(key, [writes:\{(val, version, TxID)\},  reads: \{TS, rversion, dep, TxID\}])\}
 	\subitem Dependents = \{(TxID, dependents\}
 	\subitem WaitingDeps = \{(TxID, dependencies\}
 	\item DB = \{(key, [w: \{(val, ver, TxID)\}, r: \{(TS, rv, TxID)\}])\}
 	\subitem CommitLog = \{(TxID, CommitCert)\}
 	\subitem AbortLog = \{(TxID, AbortCert)\}
	 	
 	
 	\end{itemize}
  \end{mdframed}
  \caption{Replica State and Datastructures}
  \label{fig:RS}
\end{figure}



%-------------------------------------------------------------------------------
\subsection{Execution}
%-------------------------------------------------------------------------------


Clients in Indicus both execute and submit their own Transactions. As previously defined a Transaction TX is a sequence of read and write requests that is ultimately terminated by a Commit or Abort decision. A TX object, as shown in Figure \ref{fig:TX} records the execution state necessary for Validation. The execution protocol has three goals: 1) Honest Clients should read valid data, i.e. experience read integrity, 2) Honest Clients should read fresh data, i.e. minimize staleness and hence maximize commit chance, and 3) avoid expensive coordination as much as possible. This is complicated by the presence of byzantine replicas as well as concurrent transactions. Byzantine replicas may provide invalid or arbitrarily stale data while honest replicas may be temporarily out of sync.

We avoid invalid reads (read values that were never committed) by requiring replicas to provide a proof of validity, i.e. a Quorum of signatures confirming the committment. In order to minimize coordination only read requests incur a network rountrip, while writes are buffered locally until Validation. 

Client execution conducts as follows:

\begin{enumerate}
\item \textbf{Begin()} A client begins a Transaction by optimistically choosing a timestamp $TS \coloneqq (Time, Client ID)$. 
\item \textbf{Write(key, value)}. A Client executes a request Write(key, value) by locally buffering (key, value) and returning. Concretely: $WriteSet = WriteSet \cup (key, value)$

%%%%%%%%%Read protocol%%%%%%%%%%%

\item \textbf{Read(key, TS, RQS)} 

\fbox{\begin{minipage}{21em}
\textbf{1: C} $\rightarrow$ \textbf{R}: Client sends read request to Replicas
\end{minipage}}

Given hyperparameter Read Quorum Size (RQS), a Client performs a read on given key at timestamp TS by reading from RQS different replicas. To do so, a Client sends $Read \coloneqq (key, TS)$  to $RQS$ different replicas. Note, that in order to guarantee $\geq RQS$ replies a client might need to send up to $f$ additional requests to compensate for unresponsive/faulty participants ($max(|Replies|) \leq n-f$). \fs{this does not match the rest of the discussion (RQS > n-f), or read locks- say your guaranteed to receive RQS-f} 
\fs{The RQS discussion is currently quite confusing:}
\fs{CID is part of TS, does not need to be signed if not implementing access control}\\

\fbox{\begin{minipage}{21em}
\textbf{2: R} $\rightarrow$ \textbf{C}: Replica processes Client read and returns reply
\end{minipage}}

A replica returns a signed pair \text{$\langle \textit{(Committed, Prepared)} \rangle _{\sigma_r}$} \fs{technically only Prepared needs to be signed - more practical too}, where $Committed$ is the write with largest committed write version prior to the specified Timestamp and $Prepared$ is the respective largest uncommitted write that is pending and would make $Committed$ outdated. 


\fbox{\begin{minipage}{21em}
\textbf{3: C} ($\rightarrow$ \textbf{R}): Client receives read replies and asynchronously broadcasts dependencies
\end{minipage}}

A client waits for RQS read replies and validates the integrity of any $Committed$ it receives.  It adds the biggest $Committed$ or $Prepared$ (iff enough matching) version seen to its Read Set, and claims a dependency if it was a $Prepared$ version. If it claims a dependency, it asynchronously forwards this dependency to all replicas. 
\{send dependency set, does not need to be proven.\}



\fbox{\begin{minipage}{21em}
\textbf{(4: R)} : Replica receives dependencies and adds exceptions
\end{minipage}}
Upon reception of $\langle (key, TS, dep)\rangle_{\sigma_c}$ a replica adds an exception to its Read Timestamp Set. Concretely, $RTSS(key)(TS) += dep$. \fs{client message signed in order to avoid other clients from claiming false exceptions; technically not necessary if we assume other clients do not know for keys are being read}



\item \textbf{Commit() aka Validate} A Client finalizes its execution, computes the final $TxID \coloneqq H(TX)$  and submits the Transaction for validation. 

\underline{Additonal subtlelties}: Computing the transaction identifier based on a hash of the execution object guarantees that a byzantine client may not equivocate transaction contents. The ID serves both as common identifier for remaining protocol components, and as validation tool for the integrity of a transaction object.

\item \textbf{Abort() aka Withdraw} A client terminates execution, broadcasts a read-release for all potentially acquired Read Timestamps (RTS) and returns.

\underline{Additonal subtlelties}: A byzantine client may never release its RTS. For instance, it may perform reads just for the sake of acquiring RTS without any intention of committing. We discuss mechanisms to sidestep this in section X (Optional Modifications).

\end{enumerate}


We remark, that a byzantine client is not required to follow any of the protocol steps, with the exception of claiming dependencies. By our definition of Byzantine-Isolation, read integrity is only required for honest clients; Byzantine clients may \textit{choose} whether to read legal, or even real data. Thus, we need not require for clients to prove the correctness of their reads to replicas during validation. We do, however, require that no client can claim dependencies on uncommitted transactions that have not been observed by at least one honest replica. This allows us to only include only transaction identifiers as dependencies, instead of the entire depending transaction, thereby minimizing the common path overhead. When the full transaction information is required, i.e. to complete potentially stalled transactions, it can be obtained with guarantee from an honest replica. We discuss how to complete stalled or slow transactions in section Y (Granting Liveness).

\textit{Aside:} While we can enforce access control on write transactions, we cannot do so for reads, as any byzantine replica perform this service. Solving this problem is beyond the scope of this work, and we defer the reader to existing solutions relying on multi-party computation and shared secret techniques \cite{basu2019efficient}.

%-------------------------------------------------------------------------------
\subsection{Concurrency Control}
%-------------------------------------------------------------------------------
Next, we discuss the concurrency control necessary to maintain Byzantine-Serializability. Since Indicus clients execute optimistically concurrently, Isolation conflicts might arise. In order to reconcile conflicts we need to design an appropriate Validation check that assures that no two conflicting transactions may commit. In this section we discuss the design of our concurrency control check (CCC). In section X (Validation) we then we discuss how to coordinate this check in a replicated but on-ordered setting.

The goal of the concurrency control mechanism is to maximize the legal commutativity between transactions (i.e. maximize commit rate for concurrent transactions) while maintaining our desired Isolation level, Byzantine-Serializability.
Our starting point is a traditional Timestamp Ordering approach (TSO) \cite{zhang2015tapir, adya1995efficient} in which each transaction is assigned a speculative timestamp \textit{before} Validation. This allows us to a) evaluate read/write conflicts on the basis of their pre-defined order, i.e. only classify them as conflict if their arrival order violates the timestamp order, and b) avoid write/write conflicts alltogether by simply ignoring obsolete writes \cite{thomas1979majority}(Thomas Write Rule). 

When execution results match the pre-defined timestamp order, no aborts are necessary. Our challenge then becomes to 1) assign appropriate timestamps and 2) coordinate execution in a way that maximizes such coherence; the presence of byzantine participants (clients/replicas) complicates this.

1) First, we require a timestamping service that (accurately) reflects the real-time execution start. While we assume that clocks are loosely synchronized across honest participants, byzantine participants may diverge arbitrarily in undesirable fashion. We compromise by allowing clients to optimistically select their own timestamp, but rejecting timestamps above some Threshold. We point out, that both choosing a timestamp that is too small or too large increases the risk of ones own transaction being ignored, thus incentivising clients to select real-time timestamps. 

2) Second, in order to maximize coherence between execution and timestamp we desire to minimize both execution and commit (validation \& writeback) time. The smaller the window for out-of-order interleavings, the higher the rate to commit. As previously alluded to, writes are buffered locally, while reads may require WAN roundtrips to provide integrity. This increases the window for conflicting writes to arrive, thus causing read transactions to abort.
Moreover, achieving replication (Validation/Writeback phase) requires additional roundtrips in order to maintain consistency. Consequently, write transactions become committed only after a long time, resulting in successive concurrent reads arriving out-of-order.
To mititage the frequency of read aborts we add three mechanisms on top of TSO, resulting in and implementation of MVTSO (Multiversion TSO):
\begin{itemize}
\item Multiversioning: We store multiple past write versions and allow read operations to "read in the past". This avoids unecessary aborts when read transactions execute only \textit{after} a write with higher timestamp. Previously, long executing read transactions (i.e. multiple sequential reads) could have suffered from a) reading from inconsistent database snapshots due to concurrent writes and b) from their timestamp growing smaller and smaller relative to concurrent write transactions validating at the same times. \fs{fix these last sentences}
\item Read Uncommitted: We allow read operations to optimistically read locally validated, but not fully committed transactions. By making writes \textit{visible earlier}, unecessary aborts for reads that would have previously missed the newer write are avoided.
\item Read Timestamps: We allow read operations to claim \textit{Read Timestamps} by signaling replicas to abort out-of-order writes (writes with smaller timestamps). This fairness policy implements a bias towards read operations, by aborting writes instead of reads. Since most workloads are read dominated and write-only transactions can re-execute more swiftly, this is an intuitively sensible trade-off. In section Y (Retries) 	we discuss an optional modification to support read-modify-write transactions. \fs{retries}
\end{itemize}


Note, that for write heavy workloads, or workloads without read/write conflicts MVTSO degenerates to TSO.

Algorithm \ref{mvtso} shows the necessary validation check to preserve Byzantine-Serializability. 
Given a transaction TX finalized execution state, i.e. its read-set, write-set and dependencies, and a proposed timestamp, the validation check returns \textit{Abstain/Abort} (we discuss the distinction further in section X (validation)) if a conflict has been detected. Otherwise, a replica tentatively \textit{Prepares} a transaction by adding its reads and writes to the $PreparedDB$, making its writes visible and evaluating future transactions against it for conflicts. Regardless of the outcome, a replica garbage collects all Read Timestmaps (RTS) associated with the transactions reads (indexable by matching TS).
It then waits for necessary dependencies (uncommited writes that were read) to be resolved. \fs{(ADD note: dependency blocking only needs to happen at the shard where the dep came from)} \fs{should unprepare if dep aborts; this avoids more dependencies to be formed unecessarily}
If all dependencies resolve do so successfully the check returns Commit, and otherwise un-prepares the transaction and returns Abstain/Abort. We remark, that the concurrency control check is serialized and executed atomically for each transaction.


\begin{algorithm}
\caption{MVTSO-Check(TX, TS)}\label{mvtso}
\begin{algorithmic}[1]
\If{\textit{$TS > localClock + \delta$}} %  || $TS < lowWM$ || $\exists d \in dep: d.TS < lowWM$} } dont mention garbage collection part here, it only confuses
\State \Return ABSTAIN
\EndIf


%Should be PreparedDB and DB for lookups: Check: exists read with TS and version. I.e.: Exists (TS) in DB[key][writes] and (TS, rv) in DB[key][reads]
\For{\textit{$\forall key,version \in \textit{TX.read-set}$}}
        \If{$ \exists TX2 \in CommitLog: key \in \textit{TX2.write-set} \land version < \textit{TX2.TS} < TS$}  
          \State  \Return ABORT, \textit{TX2, TX2.CommitCert}
        \EndIf
         \If{$ \exists TX2 \in Prepared: key \in \textit{TX2.write-set} \land version < \textit{TX2.TS} < TS$}   
          \State  \Return ABSTAIN, \textit{TX2}
        \EndIf
          
   %     \If{$ \textit{dep[key]} == null \land version \notin prepared-reads[key] \cup  CommitLog \cup AbortLog $}   
    %      \State  \Return $\textit{Proof-of-Misbehavior}$
    %    \EndIf
		   
        
        
\EndFor

\For{\textit{$\forall key \in \textit{TX.write-set}$}}
        \If{$\exists TX2 \in Prepared/CommitLog : \textit{TX2.read-set[key].version} < TS < TX2.TS$} % \land TX \notin TX2.dep  ..Not necessary: if the read depends on our write for this key then it is implied that the version is not smaller.
          \State  \Return ABSTAIN, \textit{TX2} || ABORT, \textit{TX2, TX2.CommitCert}
          %\State  \Return RETRY, %$\textit{TS_{new} | \forall TS_{new} > TX2.TS}$ // 
        \EndIf
        \If{$\exists RTS \in key.RTS: RTS > TS \land TX \notin RTS.dep$} %Here you do not know about the version, so you must include the exception on the dep
          %\State  \Return RETRY, %$\textit{TS_{new} | \forall TS_{new} > TX2.TS}$ // 
          \State  \Return ABSTAIN
        \EndIf

\EndFor
\State Prepared.add(TX) 


\While{$\exists d \in dep: d \notin CommitLog \cup AbortLog $)}
\State Wait
\EndWhile

%structure it in a way that is better
\For{\textit{$\forall d \in dep$}}
		\If{$ d \in AbortLog $}
			Prepare.remove(TX)
			\State \Return ABORT, d.AbortCert
		\EndIf
		
		%%%%%%%%%%%%Only relevant to retries
        %\If{$ d \in CommitLog $}
        %	\If{$d.TS > TS$}
        %	\State \Return ABORT, d.CommitCert  %% Remove because it is only a Retry optimization
        %  	\EndIf
       
		%\Else 
		%\State \Return ABORT, d.AbortCert
		%\EndIf
\EndFor


\State \Return COMMIT


\end{algorithmic}

\end{algorithm}

\paragraph{Replica State}
\fs{Need to modify MVTSO pseudocode to match the replica strucutres? or is the way it is expressed rn better.}
In order to perform the MVTSO-check (efficiently), a replica maintains several data strucutres as seen in Figure \ref{fig:RS}): It keeps track of its \textit{Database}, of \textit{Ongoing} transactions, and \textit{Completed} transactions. The Database is manifested as multi-versioned key value store. It contains both the writes and reads of all committed transactions.
Further, completed transactions are logged in respective Commit and Abort Logs, that effectively represent the ledger of all processed transactions. Since replicas process transactions out of order, we store this log as a set. This allows us to perform swift lookups for dependencies as transactions can be identified by their TxID. 
Replicas keep track of ongoing transactions and their respective validation protocol state (see section Y). Additionally, it maintains a temporary key-value store $PreparedDB$ of tentatively committing (prepared) transactions to service both optimistic reads and check for conflicts. 
When a transaction performs the MVTSO-check, its dependencies may not yet be present in either Commit or Abort logs. To avoid busy waiting, a replica temporarily suspends the MVTSO-check for the current transaction until all dependencies are resolved, allowing it to process other transactions pending validation. 
In section X (Writeback) and Y (Garbage Collection) we discuss how to garbage collect ongoing transaction state and completed transaction state respectively. 




\begin{theorem}
The set of transactions for which the MVTSO-Check returns Commit is Byzantine-Serializable. 
\end{theorem}


\textit{Aside:} Consistent with our definition of Byzantine-Isolation, byzantine clients may issue ficticious Read-Sets comprised of arbitrary read versions and values. However, these have only limited external effect on concurrent writes. We distinguish two extreme cases: 1) $read.version \rightarrow 0$: This case is equivalent to simply reading stale data, and effectively reduces MVTSO to TSO as conlflicts are evaluated only on basis of the transaction timestamps (i.e. Abort write if: write.TS < read.TS). 2) $read.version \rightarrow read.TS$: In this case there are no conflicts as a write is never "missed" by a previous read.


In the following section we will show how to design a replicated validation scheme that upholds Isolation guarantees and maintains consistency even when replicas validate out-of-order.




%-------------------------------------------------------------------------------
\subsection{Validation}
%-------------------------------------------------------------------------------

The goals of the Validation phase design are threefold: i) It needs to preserve Isolation guarantees between transactions, ii) It should embrace partial ordering \fs{weird}, while minimizing commit latency and  maximizing scalability, and iii) it should be \textit{leaderless}. Satisfying these goals requires ovecoming several challenges. In order to maximize parallelism and embrace partial ordering, Indicus allows replicas to process requests out of order. Consequently, replicas may temporarily be out of sync, and hence return different results. Such divergence must be reconciled in a way that maintains Isolation, but not overly conservatively in order to maximize the ability to commit successfully and bound the impact of byzantine participants. Further, Indicus designates clients as validation coordinator for its own transactions. The respective protocol must tolerate client failures such as crashes, omission/stalling, equivocation, or replays and allow for consistent recovery. 

The Validation protocol can be broken down into two functionalities: Voting and Logging.
Since Indicus aims to prescribe a partial order, Replicas need to agree on results, rather than an Ordering. To do so in a leaderless fashion, they vote. As in any democratic decision, these votes are then aggregated into a decision. In the case of Indicus, the client acts as the transaction coordinator who aggregates and relays results (along with necessary evidence).
 In order to maintain consistency in the presence of failures (i.e. no two honest replicas finalize different Commit/Abort decisions), this decision must be logged prior to returning, guaranteeing that a replay of any Writeback is idempotent. \fs{This two step decision protocol resembles Two-Phase Commit.} However, since we cannot trust a byzantine coordinator to persist a decision, nor could we retain liveness during a crash, we delegate the decision logging to the replicas.
 
The voting phase requires a single round-trip to all replicas, whereas the logging phase requires at most one round-trip \fs{in Indicus5 and at most two round-trips in Indicus3.}. When execution is \textit{gracious} Transactions can be committed on the \textit{Fast-Path} in a single-round trip as an explicit logging round is not necessary. Figure \ref{fig:ValO} gives an overview of the protocol.

\begin{figure}

\includegraphics[width= 0.4\textwidth]{./figures/Validation.png}

\caption{Validation Protocol}
\label{fig:ValO}
\end{figure}

\begin{figure}[t]
  \begin{mdframed}[roundcorner=10pt]
 	\textbf{TX Val State}
 	\begin{itemize}
 	\item TxID
 	\item originalClientID
 	\item interestedClients = \{CID\}
 	\item ClientSeqNo
 	\item Transaction = TXExecState
 	\item vote   ~~~~~~~~~~//Prepare1R
 	\item decision = (dec, view.no) ~~~~~~~//Prepare2R
 	\item optional: dec proof = \{$votes_r$ \} ~~~ //Only for Retries and Indicus3
 	\item current.view
 	\end{itemize}
  \end{mdframed}
  \caption{Transaction Validation State at Replicas}
  \label{fig:Val}
\end{figure}

To describe how the protocol operates in detail we follow a single-shard transaction through the system. Figure \ref{fig:Val} shows the protocol state replicas maintain for each ongoing transaction.

\fbox{\begin{minipage}{21em}
\textbf{(1: C $\rightarrow$ R)}: Client sends Prepare request to all Replicas within the Shard.
\end{minipage}}

Upon deciding to Commit in the Execution phase, a Client initiates Validation by sending a message $Phase1 \coloneqq \langle Prepare, TxID, TX \rangle_{\sigma_c}$ to all Replicas.

\underline{Additional subtlelties}: A client may not equivocate this request. Any change to the TX state changes the TXiD and hence is treated as seperate TX. 

\fbox{\begin{minipage}{21em}
\textbf{(2: R $\rightarrow$ C)}: Replica receives validation request, processes it and returns vote to Client.
\end{minipage}}
A replica validates Timestamp, ReadSet and Dependencies' integrity of the request. It then evaluates Read and Write Sets for Isolation conflicts using the MVTSO Concurrency Control Check (CCC), as shown in algorithm 1. The concurrency check is entirely based on local knowledge of previously committed, and concurrently ongoing transactions. If successful, a replica updates its set of potenially committable transactions (Prepared) and returns a Commit Vote to the client. If unsuccessful, a replica returns an Abort/Abstain Vote to the client, along with evidence justifying the decision. It sends a message $Phase1R \coloneqq \langle TxID, vote \rangle_r$.




\fbox{\begin{minipage}{21em}
\textbf{(3: C)}: Client waits for vote replies.
\end{minipage}}
A client waits for at least $n-f$ ($4f+1$ in Indicus5) distinct replica votes, or more, up to a system specified timeout. 

\fbox{\begin{minipage}{21em}
\textbf{(3a: C)}: Client receives Threshold of matching votes and returns to application. Proceeds to Writeback
\end{minipage}}
In any of the following 3 cases, a client may short-circuit waiting for additional votes and omit a dedicated Logging round:
\begin{enumerate}
\item \textbf{$1$ Abort vote w/ Conflicting TX \& CommitCertificate}: The client validates the integrity of the CommitCertificate (CC) and returns the shard decision $(TxID, Abort, \langle ConflTX \rangle_{CC})$. 

\item \textbf{$3f+1$ Abstain votes w/ Conflicting TX}: The client returns the shard decision $(TxID, Abort, \{\langle AbstainVote\rangle_r\})$. 

\item \textbf{$5f+1$ Commit votes}: The client returns the shard decision $(TxID, Commit, \{\langle CommitVote \rangle_r\}$
\end{enumerate}
Any of such Quorums forms a \textit{Shard-Certificate} and proves the decision. A client uses this Certificate to return to its application and issue the Writeback.
\fs{need to distinguish with Indicus3: can still abort FP, but not commit.}

\fbox{\begin{minipage}{21em}
\textbf{(3b: C $\rightarrow$ R)}: Client receives inconsistent results and suggests a consistent decision to Replicas for Logging
\end{minipage}}
If a client does not receive the necessary thresholds of votes to return, it must continue on the \textit{Slow-Path}. To do so, it aggregates the votes according to a conservative \fs{not really} decison rule:
If there exists a $CommitQuorum \coloneqq \frac{n+f+1}{2}$ ($3f+1$/$2f+1$ in Indicus5 and Indicus3 respectively) of Commit Votes, the Slow-Path decision is Commit, otherwise it is Abort.
A client broadcasts a message $Phase2 \coloneqq (TxID, decision)_c, \{\langle Phase1R \rangle_r\}$.


\fbox{\begin{minipage}{21em}
\textbf{(4: R $\rightarrow$ C)}: Replicas receive, validate and echo decision
\end{minipage}}
A replica confirms that the Decision matches the Quorum, by evaluating the decision rule itself. It then returns the decision to the client by sending $Phase2R \coloneqq \langle TxID, decision \rangle_r$. \fs{with retries this needs to include the timestamp}


\fbox{\begin{minipage}{21em}
\textbf{(5: C)}: Client returns shard-decision to application and proceeds to Writeback
\end{minipage}}
A client waits for a Quorum of $n-f$ matching Phase2 Replies. Such a Quorum forms a \textit{Shard-Certificate} and proves the decision. A client uses this Certificate to return to its application and issue the Writeback.

\underline{Additional subtlelties}: If a client equivocated, it will never receive a Shard-Certificate. An honest client however, is guaranteed to receive matching Phase2 replies. 


We now briefly allude to the correctness of respective decision rules and Quorum sizes.


We consider a decision (Commit, Abort) to be \textit{logged} when it is possible for some Shard-Certificate to exist, i.e. as soon as the necessary certificate Quorums exists at some Replicas.
Figure \ref{fig:FigureSP} summarizes the relevant nomenclature.

\begin{figure}
\begin{center}
\includegraphics[width= 0.5\textwidth]{./figures/Nom2.png}
\end{center}
\caption{Validation Nomenclature, Slow-Path. Note, that a byzantine client may equivocate Phase2 decisions by including Commit and Abort Quorums respectively. Byantine replicas may store multiple votes and decisions.}
\label{fig:FigureSP}
\end{figure}

\subsubsection{Correctness}
We show, that a \textit{logged} decision is final:
\begin{theorem}[Saf]
A logged decision is durable, and there can ever exist \textbf{at most one} logged decision.
\end{theorem}
\begin{proof}
We show this by case distinction. Slow-Path: A Slow-Path decision is \textit{logged} if $\frac{n+f+1}{2} = 3f+1$ ($LoggedQuorum$) honest replicas have adopted the decision, since $n-f = 4f+1$ votes suffice to form a Shard-Certificate and $f$ byzantine participants may decide arbitrarily. Thus it is impossible for two logged decisions to co-exist, as any two $LoggedQuorums$ must intersect in $f+1$ honest replicas that will only ever accept one decision - a contradiction. Furthermore, honest replicas do not change their decision, and hence a slow-path logged decision persists. Fast-Path: We distinguish three sub-cases. The existance of $4f+1$ Phase1 honest commit votes, implies that any Slow-Path decision must result in a commit decision, since any Quorum ($n-f = 4f+1$) is bound to include $3f+1$ commit votes. Vice versa, the existance of $3f+1$ honest Phase1 abstain votes, implies the impossibility of any Slow-Path commit decision. Moreover, both the above cases mutually exclude each other. Lastly, 1 valid Abort vote implies the existance of a logged decision for the conflicting Transaction. By Induction \fs{that TXs logged decision never changes}, and Quorum intersection, this implies that at least $3f+1$ honest replicas will vote to Abstain the ongoing transaction and hence it is impossible to ever log a commit decision for the ongoing Transaction.
\end{proof} 

Note, that since replicas never change their decision, it is possible for there to never be any logged decision if a byzantine client equivocated its Slow-Path Quorums. In order to reconcile this, we design and discuss a recovery mechanism in section X which relaxes the requirement on persisting a decision.  


\begin{theorem} 
Indicus maintains \textit{Byzantine-Serializability}.
\end{theorem}
To prove that this is the case, we show that for any two conflicting transactions, at most one can be committed.
\begin{proof}
Let TX1 be a transaction with logged decision Commit \fs{, i.e. TX1 has either already committed at or is bound to commit at all honest replicas}. Let TX2 be a conflicting transaction, that if committed, would violate Byzantine-Serializability. Assume TX2 too, managed to log a commit decision. By the protocol (and proof of Theorem Y -the above theorem), at least  $\frac{n+f+1}{2} = 3f+1$ commit votes are required to log a decision, and no honest replica changes its vote. By Quorum intersection, at least one honest replica must have voted commit for both TX1 and TX2. WLOG, this replica received TX1 before TX2, and, by the correctness of the MVTSO-check \fs{Theorem in CC section}, must have voted Abort or Abstain for TX2. A contradiction.
\end{proof}




\begin{theorem} 
Indicus maintains Byzantine Independence in the absence of network adversary.
\end{theorem}

We show, that once a Client submits a transaction for validation, the result cannot be unilaterally decided by any byzantine participant, be it client or replica.
\begin{proof}
First, we observe that a client may never choose a result itself, but only implicitly influence a decision by choice of Slow-Path Quorum. Specifically, a byzantine client cannot single-handedly decide to abort its own transaction and consequently, cannot single-handedly force potentially dependent transactions to abort as well. \fs{this is not quite true, need to expand on it}
Second, any Quorum decision requires at least one honest replicas vote. In particular, a $f$ byzantine replicas may vote to abstain arbitrarily (by always reactively generating a ficticious transaction), but at least one additional honest abort vote is necessary to result in an abort (at least $f+1$ additional honest votes if the network is synchronous with regards to the timeout $\delta$). 
Thus, in order to artificially cause transactions to abort, a conflicting transaction must be generated (artificial congestion). However, to do so strategically and reliably \fs{deterministically}, the adversary must control the network in order to guarantee the artifical transactions arrival at honest replicas \textit{before} the original transaction. 

\fs{likewise 2 byz clients may not abort each other:Consider as a more subtle case the scenario in which two colluding byzantine clients attempt to abort each other by sending 2 tx that either conflict (or the one depends on the other) to different subsets of replicas in different order. In order to do so they need to control the network. If they DO manage to set this up, they could abort any transaction that ends up depending on either of them. However to do so deterministically, they need to set up accordingly BEFORE the honest TX arrives (which requires network control) and KNOW the keys to do so strategically (which we assume they dont: see properties section assumption);  1 byz client could issue two tx that arrive only in fifo order so its fine too (more than some threshold t of concurrent tx will be counted as PoM)}
Thus, when the network is not adversarial, validation decisions are \textit{Byzantine Independent}.
\end{proof}

%-------------------------------------------------------------------------------
\subsection{Writeback and Multi-shard 2pc}
%-------------------------------------------------------------------------------

Validation occurs on every Shard that a transaction spans. The goal of the Writeback phase is to aggregate all relevant shard-decisions and to inform replicas of finalized Commit or Abort decisions. This is necessary in order for replicas to be able to garbage collect meta-data of ongoing transactions, and to allow consecutive transactions to reliably observe the updated state. Any finalized decision must respect all shard-decisions relevant to a transaction. Thus, all decisions are aggregated according to standard two-phase commit. Only if all shards agree that a transaction may commit (i.e. there exist commit certificates for every shard), then a transaction may commit. The protocol is simple:

\textbf{1. A coordinator waits for all shard decisions, including certificates}\\
\textbf{2. The coordinator broadcasts a $Writeback \coloneqq (TX, decision, \{certificates_S \} )$ message to all replicas in all relevant shards}.\\

\fbox{
\begin{minipage}{21em}
\textbf{(1 : Coord $\rightarrow$ S)}: A coordinator aggregates decisions and forwards them to all relevant shards.
\end{minipage}
}

A coordinator waits for all shard decisions, including certificates. The coordinator aggregates the decisions and broadcasts a $Writeback \coloneqq (TX, decision, \{certificates_S \} )$ message to all replicas in all relevant shards.
\underline{Additional subtlelties:} A transaction includes a set \textit{InvolvedShards} of relevant shards . A \textit{Writeback} message is only valid if there exists a shard-certificate for every involved shard. The Writeback message does not need to be signed. A single Abort certificate may short-circuit the coordinator wait as it suffices to Abort.\\


\fbox{\begin{minipage}{21em}
\textbf{(2 :  S)}: Replicas in a shard validate and finalize the writeback decision.
\end{minipage}}
Upon reception of a \textit{Writeback} messgage, a replica validates whether its own shard is involved, and if so whether a) there exist a single Abort shard-certificate, or b) Commit shard-certificates for all involved shards. If not, a replica rejects the request. If the request is valid a replica garbage collects all ongoing transaction state, and applies the state updates if the decision was Commit.\\


We point out, that the Writeback coordinator need not be the client issuing the transaction, but can in fact be an arbitrary party (client or replica) that is interested in completing the Writeback. This follows straightforwardly from Theorem Y: Any certified shard-decision implies the existence of a logged decision, and hence the Writeback phase is idempotent.
We utilize this to drive the recovery protocol outlined next. 

\paragraph{Optimization: Single shard logging}
\fs{This does not work for Atomic Broadcast!}

When transaction execution touches multiple shards validation can incur redundant explicit logging overhead. When a Slow-Path is necessary to arrive at a logged decision on S different shards, bandwith is wasted. Consider an example in which $S-1$ shards attempt to log the decision Commit, while a single shard attempts to log an Abort decision. If the latter shard succeeds, the effort of the remaining shards was in vain. 
The culprit of this phenomenon is the delayal of Two-Phase-Commit (2PC) until the Writeback phase. By preemptively making a 2PC decision \textbf{before} logging we can avoid this redundancy. We remark, that even when when all shards agree on a decision, this saves redundant coordination.
Concretely, we designate \textbf{one} involved shard as \textit{logging Shard}, while all other shards remain responsible only for Validation. Figure \ref{fig:SingleShardOpt} shows the revised structure. In order to log a decision, voting quorums from all involved shards are required. We modify step 3 of the Validation protocol accordingly:

\fbox{\begin{minipage}{21em}
\textbf{Validation (3: C)}: Client waits for vote replies from all involved Shards.
\end{minipage}}
For each involved shard, a client waits for at least $n-f$ ($4f+1$ in Indicus5) distinct replica votes, or more, up to a system specified timeout. A client aggregates a per-shard decision for each shard according to the \textit{CommitQuorum} rule. If all shard-decisions are Commit, it attempts to log a Commit decision by sending $Phase2 \coloneqq (TxID, Commit, S \times \{CommitQuorum\}$ to all replicas in the designated logging Shard. If a single shard-decision is Abort, it stops waiting for other shard-decisions and attempts to log an Abort decision by instead sending $Phase2 \coloneqq (TxID, Abort, AbortQuorum)$. 

\underline{Additional subtlelties:} A client can go Fast-Path and return to the Writeback phase immediately only if Fast-Path Quorums were received for all shards. Logging is always bottlenecked by the slowest shard. The logging Shard can be determined via a determinsitic function over the \textit{involved Shards}. A simple solution can designate the shard with lowest ID. A load balanced function could select $loggingS = min(TxID \% involvedShard)$.

The remaining Validation protocol proceeds identically to the multi-shard version. Notice, that when only a single shard is involved, no adjustments were made. The Writeback phase instead, may proceed with just the single certificate from the logging Shard: 

\fbox{
\begin{minipage}{21em}
\textbf{Writeback (1 : Coord $\rightarrow$ S)}: A coordinator receives certificate and forwards them to all relevant shards.
\end{minipage}
}
A coordinator waits for the logging shard decision, including shard-certificate, and broadcasts a $Writeback \coloneqq (TX, decision, certificate_loggingS )$ message to all replicas in all relevant shards.

The Fallback protocol is adjusted accordingly: A fallback replica need (and can) only be elected on the logging Shard, simplifying reconciliation and reducing the cost for interested clients. 
To further reduce unecessary load, a client may attempt to first inquire whether decisions exists at the logging shard (Fallback protocol step 1 \& 2), before sending $Rec-Phase1$ messages to all shards in order to gather votes itself (Fallback protocol step 1). 


\begin{figure*}
\begin{center}
\includegraphics[width= \textwidth]{./figures/SingleShard.png}
\end{center}
\caption{Single Shard Optimization}
\label{fig:SingleShardOpt}
\end{figure*}

%-------------------------------------------------------------------------------
\subsection{Granting Liveness}
%-------------------------------------------------------------------------------

Indicus operates under the premise that clients experience progress indepentently. Since agreement occurs on a per Transaction basis (each client drives its own validation) and replicas may process requests out of order, there exists no shared notion of system progress. When all participants are honest progress is trivially guaranteed for every client. Byzantine clients however, may bring execution, validation or writeback to a halt for their own transactions. For example, a byzantine client may either stall during all phases, or equivocate during validation logging. The latter case lets replicas diverge on the decision value (Commit/Abort), making impossible to arrive at a shard-decision, and hence degenerates to a form of stalling too. When all transactions are commutative this phenomenon requires no action: Any client \textit{chooses} whether to adhere to the protocol and experience progress, or not. However, when this is not the case, and transactions depend on each other, either explicitly (through reading anothers transactions uncommitted write), or implicitly (through read-write conflicts), liveness is no longer independent. For instance, a claimed dependency might be of byzantine origin (explicit dependency) and never terminate, causing the dependent to stall helplessly itself. Alternatively, an uncommited write, yet unclaimed dependency, (implicit dependency) might forever cause all consecutive read transactions to abort .
Thus, in order to allow clients to intertwine their fate with concurrent transactions while remaining in charge of their own destiny, we must strengthen Theorem X. We define Liv:

\begin{theorem}[Liv] \fs{change to: there should exist a shard-decision?}
For any transaction, there exists \textbf{exactly one} logged decision, if desired by an honest client.
\fs{Any request that an honest client is interested in (broader than just "issued by an honest client") eventually completes}
\end{theorem}
Intuitively, this property allows honest clients to experience progress, if they desire to. To achieve this, we must relax the requirement that replicas may never their decision, while preserving Theorem X (rename to Safety).


A naive solution would allow any client to drive another clients protocol. This is problematic, since \textit{interested} clients could concurrently make inconsistent decisions and consequently never make progress \fs{an all to all between replicas has the same behavior as multiple clients potentially, need single leader to guarantee that all agree on Quorum}. Electing a single client is similarily not live, as there is an unbounded number of byzantine clients that will constructively aid in reconciliation. We circumvent this, by letting concurrent clients replay the protcol, but partially delegating responsibility to the replicas. Concretely, we design a mechanism to elect \fs{endorse ;)} a dedicated \textit{Fallback} replica that is responsible for reconciling diverged replica decisions. Since the number of faulty replicas is bounded, at most $f+1$ leader elections are necessary to make progress when the network is synchronous. A challenge in doing so is to guarantee a live round-robin election. We remark, that in an asynchronous network, leader election might not be possible \cite{fischer1985impossibility}. 

On a high level, the recovery mechanism operates as follows: \textit{Interested} clients attempt to run the validation protocol themselves. If a client notices or suspects previously present decision divergence, it issues a \textit{Fallback invocation}, i.e. a request to elect a \textit{Fallback} replica to seize control and reconcile decisions. Such recovery protocol is reminiscent of \textit{view-changes} in traditional BFT SMR protocols, but differs in two core aspects: a) While control changes between fallback replicas (leaders), the protocol is still fully client driven (there are no \textit{view-changes} without client invocation) \fs{client does p1, p2, invocation, writeback; It is fully linear, All to all just for practical use against byz clients}, and b) The fallback mechanism impacts only the ongoing transaction, concurrent transactions keep progressing indepentently. Figure \ref{fig:FallB} gives an overview of Fallback mechanism.

\begin{figure}

\includegraphics[width= 0.4\textwidth]{./figures/Fallback.png}

\caption{Fallback Invocation}
\label{fig:FallB}
\end{figure}


Below, we detail the protocol by following a transactions life cycle. To start the protocol we assume an \textit{interested} client is in possession of the respective transactions $Prepare$ message, signed by the orignial issuer. \fs{this info can be obtained as part of MVTSO check (f+1 must have the TXID, so an honest replica will have the full TX) for own Tx or through an abort (>f+1 abstains necessary, one honest one will return the conflicting reason - subtlelty: there may be multiple different conflicts but only one is returned; it limits how clients become interested. client learns about full PrepareTX in MVTSO check: returned to it for all TX that have not completed yet. This avoids unecessary info in the exec phase (i.e. reads including full TX)}. For sake of exposition we moreover assume that the transaction has not finalized via Writeback on any replica. We briefly discuss this case afterwards.  




\fbox{\begin{minipage}{21em}
\textbf{(1: C $\rightarrow$ R)}: Client submits backup Prepare request to all Replicas in all relevant shards.
\end{minipage}}
An interested client broadcast a message $Rec-Phase1 \coloneqq (TX.Phase1, CID)$ to all replicas in \textit{InvolvedShards} of the transaction. 

\fbox{\begin{minipage}{21em}
\textbf{(2: R $\rightarrow$ C)}: Replica receives and processes Client request.
\end{minipage}}
If a replica has not previously processed the same Phase1 message, it executes the normal validation protocol and returns the according reply to both the interested client and orignial client. It furthermore starts a time-out on the transaction and adds CID to a list of \textit{InterestedClients} for the respective transaction.
Otherwise, it skips re-execution and does the following: It adds CID to the list of \textit{InterestedClients} and replies with the transactions state stored in $OngoingTX[TxID]$. This state includes the existing $Phase1R$ reply message and, if existing, a decision value ($Phase2R$ message), as well as the \textit{view.no} of the decision. It additionally includes its current $view$.

\underline{Additional subtlelties}: A replica does not need to return $Phase1R$ if it has a decision. The $view.no$ of the original client is zero. Larger views indicate decisions made by a Fallback replica, specifically by the replica with $RID = view.no \% n$. The current view indicates which is the last $Fallback$ replica a client has attempted to elect (Zero, if none yet). Further, a replica delays the timer start until the transaction has no dependencies of its own. This avoids early client evicition and Fallback election in case progress was justifiably inhibited. \fs{re-iterate this to be in sync with mvtso check. Where to block; one could argue the Timestamp is not started because it is still waiting. Hence no replies are issued either.}

%%Fallback start
\fbox{\begin{minipage}{21em}
\textbf{(3: C $\rightarrow$ R)}: Client receives responses and either returns to Writeback or invokes \textit{Fallback} election.
\end{minipage}}

We distinguish the following two cases:

\fbox{\begin{minipage}{21em}
\textbf{(3a: C $\rightarrow$ R)}: A client receives the necessary information to proceed to Writeback.
\end{minipage}}
We consider two subcases:\\
a) If a client receives enough matching $Phase1R$ messages to fulfill a Fast-Path Threshold, it can move on to Writeback. This is safe, because these are logged decisions and imply that any potenially existing Slow-Path decision is the same.
b) Alternatively, if a client receives enough matching (in decision and view) $Phase2R$ messages to form a certificate it proceeds to the Writeback phase.\\


\fbox{\begin{minipage}{21em}
\textbf{(3b: C $\rightarrow$ R)}: A client needs to go Slow-Path or observes inconsistency and requires assistance for resolution.
\end{minipage}}

If a client receives $\geq f+1$ matching decision values and views ($Phase2R$ messages; favors Commit if two matching sets exist) it forwards those to the replicas as proof for a new $Phase2$ message (with the same decision) that it broadcasts to all replicas. If it instead receives $< f+1$ matching decisions it uses the received $Phase1R$ messages to iniate its own $Phase2$ message (identical to validation step, but signed by this client). 
Additionally, if the client received inconsistent decisions (i.e. both Commit and Abort) \fs{If they were signed by the same client, this constitutes a PoM} or $<f+1$ total decision messages that differ from its own proposed decision, it initiates a \textit{Fallback election}. To do so, it broadcasts a message $InvokeFB \coloneqq (TxID, CID, \{view\}_R$. It attaches a set of $4f+1$ signed view messages.  

\fs{refine this}
\underline{Additonal subtlelties}: A client may treat a $Phase2R$ decision as $Phase1R$ vote for the sake of making a new decision. This is safe: If the replica was honest, then this reflects that there existed enough matching votes, and if it was byzantine, it could have voted arbitrarily regardless. \fs{maybe just simpler if replicas include their phase1r as well, but this adds overhead}
 \textbf{View Change Rules:} A client sends a set of signed view messages in order to let potenially diverged replicas now what next view to vote for. However, replicas only adopt a view $v+1$ if the view set includes $3f+1$ \fs{3f+1 should be enough: satisfies that f+1 votes will be found. Can use 4f+1 instead otherwise} votes from view $v$. \textit{Vote subsumtion:} A view $v$ subsumes all prior views: I.e. a replica vote $v$ may count as a vote for all $v' \leq v$. If a byzantine client attempted to diverge the views by only invoking a view change for a subset of replicas, such matching set of views might not exist. 
Replicas that lag behind, may catch up to the maximum view $v$ present $f+1$ times. Such view vote implies, that at least one honest replica has voted for view $v$, and would have done so only, if $2f+1$ honest replicas had previously been in view $v-1$. The possible divergence can be reconciled in a single step, since there must always exist some view $v'$ for which at least $f+1$ honest votes exist in any Quorum (set of 4f+1 voteS) such that $v' \geq max(honest.views) -1 $ (see Vote subsumtion). Thus, the next view change incurs at most 1 additional Roundtrip, since all honest replicas either join max(honest.view) (no additional overhead) or max(honest.view)-1 (another roundtrip to gather $3f+1$ votes.)



\fbox{\begin{minipage}{21em}
\textbf{(4: R $\rightarrow$ C)}:  Replicas receive $Phase2$ messages 
\end{minipage}}
We break the procedure into two components:

\fbox{\begin{minipage}{21em}
\textbf{(4a: R $\rightarrow$ C)}:  Replicas replies with decision
\end{minipage}}
If a replica receives a valid $Phase2$ request (see Validation) it adopts the decision and replies with a corresponding $Phase2R$ message.

\underline{Additional subtlelties:} A replica will buffer requests from $CID \neq originalCID$ and process them only \textbf{after} the timer (set in step 2) expires. If the client request includes a proof of inconsistency (see step 3 and 4b) it will ignore the timer and process the request immediately. If a replica previously adopted a decision, it treats the new request as no-op and returns the stored decision.


\fbox{\begin{minipage}{21em}
\textbf{(4b: R $\rightarrow$ R(FB))}:  Replica additionally receives Fallback invocation and starts election
\end{minipage}}

If a replica receives an $InvokeFB$ message and the original client has timed-out it attempts to elect a new \textit{Fallback replica}. To do so, it follows the \textbf{View Change Rules} and adopts new view $current.view = v$ if $v > current.view$. \fs{effectively when a replica adopts a view, it ignores all older views. Trivial, since it only accepts the first p2 and then never changes. Only thing that changes this is a FB message from higher view.}
Replicas send an $ElectFB \coloneqq (TxID, decision, current.view)_R$ to replica $current.view + TxID \% n$. \fs{this avoids always electing the replicas in the same order - it is both more load balanced and less likely for the initial replica to always be byz.}


\underline{Additional subtlelties:} 
When byzantine clients attempt to invoke elections they may only send the request to a subset of honest replicas, thus never truly enabling an election (requires $4f+1$ elect votes). If there are no concurrent honest interested clients, this can result in replicas being skipped in consideration to become the next Fallback. Moreover, to achieve liveness, we assume a weakly synchronous model, and hence increase the timeouts for each view exponentially.
In order to avoid artificially increased timeouts and enforce a round robin election without skipped candidates, replicas can forward the $ElectFB$ message to all other replicas. If another replica receives $f+1$ such messages, it adopts the view and sends an $ElectFB$ message of its own. This ensures, that for each view, a Fallback is indeed elected (if the network is synchronous within timeout $\delta$). This optimization only aids practical progress in the face of misbehavior and is not necessary for theoretical liveness. To avoid unecessary all-to-all communication, it may only be enforced for views $v > T$, where T is some system defined threshold. \fs{We choose T = f, to avoid false client suspicion in case the first f Fallbacks were byzantine and never replied} 



\fbox{\begin{minipage}{21em}
\textbf{(5: R(FB) $\rightarrow$ R)}:  Fallback aggregates and echos election messages.
\end{minipage}}
The Fallback replica considers itself elected upon receiving a Quorum of $4f+1$ ElectFB messages. It then uses this set of messages to reconcile a decision. It broadcasts a message $FBdec \coloneqq \langle RID, dec, view, \{ElectFB_r\} \rangle_R$, including the new decision and the Elect messages as proof. Effectively, replicas compute the new decision themselves; the Fallback acts as single broadcast channel in order to guarantee that all replicas receive the same Quorum of states.

\underline{Additional subtlelties:} A byzantine Fallback may equivocate Elect Quorums. However, after at most $f+1$ fallback elections a consistent result will be derived (given Synchrony).
\textbf{Decision Reconciliation Rule:} The reconciliation rule is straightforward: $dec_{new} = majority(\{Elect.decision\})$. If a logged decision exists (implies that a shard-decision might exist), any Quorum of $4f+1$ Elect messages is guaranteed to contain $2f+1$ matching decisions (a majority). Vice versa, if $<2f+1$ replicas agree on a decision, then it is impossible for this decision to have been logged. Thus it is guaranteed that logged decisions persists and otherwise an arbitrary decision qualifies since at least one honest replica must have proposed it (and hence the decision is based on a legal $Phase2$ Quorum).
\fs{optional: If a Fallback receives $>4f+1$ Elect messages, he may choose a Quorum that favors Commit (maximizes dependencies commit chance, but gives byz clients benefit of the doubt).}
\fs{If a replica receives inconsistent decisions that were signed by the same client, this constitutes a PoM for the client}


\fbox{\begin{minipage}{21em}
\textbf{(6: R $\rightarrow$ C)}:  Replicas echo decision to interested clients
\end{minipage}}
Replicas receive a $FBdec$ message and validate the legitimacy of the Fallback replica as well as the new decision. If $FBdec.view \geq current.view$ it then updates its decision and decision view, $decision = (FBdec.dec, FBdec.view)$ and sends a $FBcomp \coloneqq \langle(Phase2R, current.view\rangle_R$ message with the respective decision to all interested clients.

\underline{Additional subtlelties:} If replicas time-out waiting for a $FBdec$ message, they forward their current view to all interested clients. If $FBdec.view > current.view$ it updates its current view as well.

\fbox{\begin{minipage}{21em}
\textbf{(7: C}: A client starts Writeback phase or restarts Fallback invocation
\end{minipage}}
An interested client waits for $\geq 4f+1$ $Phase2R$ replies up to a time-out. If it received enough \textit{matching} decisions to form a shard-certificate it proceeds to the Writeback phase. If it times out, receives insufficient, or inconsistent decisions it re-starts Fallback election by broadcasting $electFB \coloneqq (TxID, CID, \{view\}_R$, for the new set of views received.

\underline{Additional subtlelties:} Only matching decisions from the same view qualify as shard-certificate. The attentive reader may notice that Elect messages however do not include the view in which a decision was made. These need not be from matching views as follows straightforward via Induction: If there ever existed a shard-certificate, then there existed a logged decision $d$ with matching views. Thus, by the Decision Reconciliation Rule, all future $dec_{new} = d$ and hence it is safe to rely on decisions from different views (- it is in fact necessary, as the Fallback may be byzantine and not broadcast to all replicas).
A client \textit{expects} to receive $4f+1$ matching decisions and will continue to wait for decisions from the same view if the first $4f+1$ messages received match in $\geq 3f+1$ cases. Upon time-out, a client proactively starts a new election (keeps waiting regardless) in case the past \textit{Fallback} was byzantine or did not succeed in timely reconciliation. Eventually a logged decision must exist and time-outs grow enough to guarantee a client will receive $4f+1$ matching. In section X (optimization) we discuss a practical optimization.
 \\

\begin{figure}
\begin{center}
\includegraphics[width= 0.5\textwidth]{./figures/FBNom.png}
\end{center}
\caption{Fallback Scenario. 1. A byzantine client equivocates Phase2 decisions by including Commit and Abort Quorums respectively. 2. An interested client that cannot observe a Shard-decision invokes a Fallback Replica (FB) election. 3. A correct FB reconciles the decisions - A byzantine FB may repeat step 1.}
\label{fig:FigureFBnom}
\end{figure}

Figure \ref{fig:FigureFBnom} shows an example scenario requiring reconciliation, and the respective Fallback protocol message pattern.

\fs{ALTERNATIVE: Clients do not do the p1/p2 themselves, but do invocation of fallback election directly (i.e. without receiving p1r and sending p2 first). In this case replicas that elect may be missing p2 decisions and the reconciliation rule might have to issue p2 decisions based on the p1 replies included. This requires replicas to actively ignore all requests to write p2 from views < current. For seperate shard logging this is staightforward. For single shard logging however, the p2 decision requires p1 from every shard and hence every shard would need to partake in election of a fallback replica - that is messy: easier if the clients just handle it seperately.}

\fs{Optimization: (Replicas can start election right away if they have a p2). then only the first interested client had to redo p1.}
%%%%%%%
Any honest client that follows the Fallback protocol experiences \textit{Liv}. This follows straightforward from the eventual existance of an honest Fallback replica that reconciles all honest replicas. Concretely, if an honest client is interested, and the network is synchronous, at most $f+1$ fallback elections are necessary in order for a decision to be logged and for an interested client to receive a shard-decision.


We want to point out that steps 1, 2a, 3 and 4a simply correspond to the normal validation protocol. Such execution might be necessary in case the original client suffered from omission (potentially byzantine) or was simply slow (i.e. there are no inconsistencies). In order to avoid \textit{interested} concurrent and/or byzantine clients claiming power too soon and causing unecessary divergence, we grant the original client an initial window of immunity.
However, after this timer expires this transaction becomes "fair game" and anybody client can claim responsibility on termination. The orignial client too, is an interested client by default - an honest client that loses autonomy will still attempt to complete the protocol, and, if necessary elect a \textit{Fallback} itself. If the original client is byzantine, yet no other client is ever interested, a replica may eventually turn into an interested "client" itself in order to drive forward garbage collection.
Note, that it is irrelevant which client issues the $Phase1$ message. Furthermore, it is both safe and "fair" for any client to return to the Writeback phase using a Fast Path. Therefore, we enforce the timeout window only for explicit logging ($Phase2$). 

When depending on slow or byzantine transactions, clients may have to incur extra latency in order to maintain liveness. \fs{Overall, if FB invocation necessary and worst case: 1 rr voting, 3 rr FB (+1 wb) for dependencies (can be in parallel for all), 1 rr logging + 1 rr own wb.}
This is the price they pay in order to experience liveness independently from the rest of the system. On the flipside, when a clients fate is independent from other transactions, it is entirely unaffected by concurrent "view-changes". Clients that are caught equivocating or responsible for for frequent time-outs may be excluded from the system. In order to participate in a consortium database meeting performance standards is a requirement.

\fs{Fallback replicas that equivocate can be expelled. Clients that equivocate cannot be expelled because one potentially cannot tell the difference between concurrent and single client. Instead, they can get punished for being slow.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% WRITEBACK HAS ALREADY HAPPENED
\textbf{Finalized decisions:} If a replica has already received a Writeback message for a transaction it ignores all Fallback related client requests and simply returns the Writeback decision, and additionally the Writeback certificate if not yet garbage collected. A client may use either this certificate or $\geq f+1$ matching decisions to issue the Writeback for other replicas. In the following section we discuss how to perform certificate garbage collection in a manner that maintains liveness.

%-------------------------------------------------------------------------------
\subsection{Garbage Collection}
%-------------------------------------------------------------------------------
In section X (Writeback) we previously discussed how replicas garbage collect finalized \textit{Ongoing Transaction State}. \fs{A replica will eventually become an interested client in order to promote garbage collection for ongoing but incomplete Txs: I.e. if a byz only ever sent to a couple replicas, and it never caused a conflict or a fallback to be started by another client}
We now briefly allude to garbage collecting unecessary certificates as well as outdated versions.


\fs{OVERVIEW: GC versions based on watermark. Cannot accept new tx below the watermark because lookups are expensive or either not possible anymore - unclear if sth has been gc. Must still accept ongoing TX info in order to finish them safely and consistently. Only remove commit certs once all versions have been gc in order to be able to serve reads. }


\textbf{Certificates}: Honest replicas store decision certificates for three purposes: a) It allows replicas to singlehandedly service reads (Commit Cert required) b) It allows them to singlehandedly propagate Writeback decisions to assist interested Clients and c) It allows Validation to return on Abort Fast Paths by providing conflict proofs (Commit Cert).
In theory, certificates may be removed whenever, as storing them is not necessary for Isolation correctness (the existance of a certificate implies the existance of a logged decision). Replicas  must then vote to Abstain where they previously voted to Abort (we point out that the existance of a logged decision implies that voting Abstain instead of Abort is always safe), while clients can no longer rely on single replica reads. Since certificates are comprised of signature Quorums they impose costly storage overhead and are hence important to garbage collect swiftly.

However, in order to guarantee that garbage collection is \textit{final}, i.e. no state is re-opened, we need to stricten the requirement on when to remove obsolete certificates. Concretely, we must confirm that enough replicas have finalized the decision, in order to guarantee that every interested client receives $\geq f+1$ matching decisions. \fs{Otherwise, a client might receive too few p2 decisions in order to start fallback, because finalized replicas ignore requests. In this case replicas would have to "play along" and let their decision be treated as p2. This requires re-opening Ongoing TX state, that may never be closed if there is no interested honest client} To facilitate this, we add an additional client-replica exchange off the critical path:

\fbox{\begin{minipage}{21em}
\textbf{(2: R $\rightarrow$ C)}:  Replicas receive and acknowledge Writeback decision
\end{minipage}}
Upon receiving and processing a valid Writeback decision a replica returns an acknowledgement message $EchoWB \coloneqq \langle TxID, decision \rangle_R$.

\fbox{\begin{minipage}{21em}
\textbf{(3: C $\rightarrow$ R)}:  Client aggregates acknowledgements and forwards them
\end{minipage}}
The client waits for $2f+1$ matching decisions, bundles them, and forwards them to all replicas.

\fbox{\begin{minipage}{21em}
\textbf{(4: R ($\rightarrow$ R))}:  Replica receives garbage collection confirmation
\end{minipage}}
Upon receiving $2f+1$ discrete replica confirmations a replica is aware that $\geq f+1$ honest replicas have finalized the decision and hence will assist any honest client in re-issuing the Writeback phase.

\underline{Additional subtlelties:} If a replica does not receive client acknowledgements within a time-out (a low Watermark for garbage collection has been reached), it broadcasts the $EchoWB$ message to all replicas. \fs{alternatively only forward to 3f+1 in a round robin scheme, GC as soon as $2f+1$ received}

Once a replica has achieved confidence that a Writeback is sufficiently replicated it may delete certificates. However, in order to process reads singlehandedly, an honest replica will maintain Commit Certificates until the corresponding version is gargabe collected. Next, we discuss how to prune old versions from the store.



\textbf{Versions}: We keep a shared \textit{low Watermark} that denotes what versions are obsolete. Whenever \fs{it is also fine to do it less frequent} a write (on any key) occurs, we advance the Watermark to $lowWM = localClock - \delta$. For this key, we then prune all versions $< lowWM$, with the latest version being exempt in order to maintain at least one version. \fs{periodically, we may do this for all keys in order to also prune "inactive" keys that still have multiple versions.} We point out, that client reads with $TS < lowWM$ may fail once we prune the genesis version. This does not imply transaction abort, since clients are free to change their timestamp and retry a read during execution.
\fs{we might want to abstain from transactions even if we did not prune the log, because those do not offer efficient lookups for conflicts: I.e. move the abstain logic for TX to this paragraph, and only keep dependency abstains in the log section.}

\fs{Read versions below watermark: must abstain because it is unknown whether conflicting writes were already garbage collected. Write TS below watermark: must abstain because it is unknown whether conlicting reads were gc.}
\textbf{Log Entries}: We store all committed or aborted transactions in respective Commit/Abort logs. These are un-ordered sets and allow both for fast lookup during Validation as well as offline audit off all transactions. For example, an audit on transaction reads or writes may be performed in order hold byzantine clients accountable for participation in the system. Illegal reads or non-atomic writes can be discovered and serve as Proof of Misbheavior to expell a client. When audit capability is not required, we can garbage collect old transaction decisions. We can remove CommitLog entries whenver a respective version of one of its writes (i.e. the first \fs{in this case cannot service reads below which is fine} or last \fs{would require to keep track of when the last is removed}) is pruned \fs{AND the certificate is signed off for garbage collection. Might require us to broadcast our decision (unless it was already braodcasted) in order to make sure, other tentative replicas are guaranteed to finish.}. AbortLog entries instead can be removed whenever the Low Watermark increases (need not happen on every update, but periodically) \fs{AND the certificate is set to be removed}. 

\textbf{Protocol Implications}:
In order to maintain Isolation, a replica must vote to Abstain from every new transaction with timestamp below the low watermark \fs{we may already want to do this once its removed from the store, because lookup is expensive afterwards}. Such a transactions reads and writes cannot be checked for conflicts anymore, as existing conflicts might have been garbage collected already. \fs{for reads: we can be a little more fine-grained: If there still exists a version that is smaller than the low WM, then no gc has happened yet and isolation is still safe. For writes this is not the case: cannot know whether there was a read above the write TS but below the WM that has been gc. } Additionally (for the same reason), replicas must Abstain from transactions that read versions below the watermark.
Lastly, replicas must abstain from any transaction that has a dependency with $dep.version  < lowWM$ \fs{dep version is the timestamp of such tx}, as its dependency could have been garbage collected already. \fs{in this case it would block infinitely. We can be a little more frine-grained: only need to abstain if the dep is not in the commit log. If it is, there is no need to abstain}. 

\fs{Problem: Voting abstain will still open ongoing TX state for TX that are doomed to abort. Ideally we would want to just reject the transaction outright, but then other replicas that accept the request might be stuck with it forever. This technically allows a byz client to re-issue the same TX again and again (it will always be aborted). What if now a TX that previously committed, manages to abort; this would break Consistency.
Seems to suggest: We should never GC the Logs, only certificates and versions. But GC old versions and not the log does not buy you a lot. 
Can remove the TXobject once version is gc, only keep TXid, should be cheap enough.
Could have actually done this as soon as a Writeback is received. Do not need the read/write sets anymore? all the info needed is in the store:
Maybe not true, what if somebody is waiting on a dep with ID, but does not know the whole TX.
--> can only remove tx info once "everybody" has seen the writeback, i.e. on all involved shards}

\fs{replicas would also like ignore all messages with TS below the watermark, i.e. writeback messages, fallback invocations etc. TX with incomplete state at some replicas may then never be finished... problematic: consider an example where 2f+1 replicas finish writeback and GC, but all others are still pending. they would never finish: This implies that we always need to forward to enough other replicas. (i.e. all to all, can happen in the round robin fashion)}

\fs{Would need periodic Checkpoint protocol to make sure that all replicas are on the same page what to GC and what not to accept anymore}

\fs{Maybe it is just ok to remove and reject all state below some low Watermark: since it was going to be removed anyways upon completion, it is not necessary to complete. This only matters if we do not want audit.
In theory the whole db could be stalled on any given replica and thus that garbage collection would never happen - Can just periodically increase WaterMark and gc all.}

%-------------------------------------------------------------------------------
\subsection{Optional Modifications}
%-------------------------------------------------------------------------------
Next, we discuss a series of optional modifications: \fs{not necessarily optimizations: Retries, Single shard logging, speeding up shard certificates, read leases, splitting tx objects}


\paragraph{MULTI-SLOT Retries}
Clients whose initial transaction must abort due to write/read conflicts may retry their transaction without re-executing by re-submitting the same transaction object under a new timestamp. In order to avoid conflicts with their own original transaction, clients will fully abort the first transaction before issuing a new transaction. (Note, that if a client cannot abort the transaction, then no retry was necessary anyways. A client can achieve FIFO ordering by piggybacking the retrying transaction request onto the Abort Writeback message of the orginal Tx.
A byzantine client issuing pointless retries creates no additional external effects, as no new read timestamps are acquired, and all retries are aborted). In order to guarantee, that a byzantine client does not re-issue the same transaction repeatedly (to cause processing load) replicas drop all transactions whose digest (TxID without Timestamp ) matches a previously committed (in this case just reply with stored result -its a replay after all if seq no matches) or pending transaction \fs{in general we only allow one active TX at a time - A client can of course re-execute the same Tx procedure multiple times, but it will presumably result in different read versions .. but technically it doesnt need to, so we would need to accept duplicate TX: imagine example where you try to read from higher TS, read the same old verions, but now SHOULD have seen newer)}. \fs{Enforce retry limit on same TX seq no.}  \fs{Since these retries only look like retries from the clients perspective - from the replicas perspective it is simply a new TX (byz client just changes seq no...) - we cannot stop byz clients from re-issuing as often as they want; this is NO different however, than them just inventing read sets, so its no additional damage}

\fs{Just dont evaluate conflicts against own ID? (Treat TXid = (hash, TS)) In this case a byz client may do double commits, but that is fine: does not mess with database state + is a soft PoM (could have lost control mid-retry) The TX will look illegal in the log?. Up to a limit of retries - dont accept past it. If its the same with diff seq no. then we dont protect against it. Replicas will reject multiple concurrent TX request unless they are for the same ID} 

Can I maintain abstains somehow to avoid double commits? -> This would still not avoid, it would just make the retry harder.

For write conflicts (line 9-10 in \ref{mvtso}) replicas return Retry and a suggested Timestamp (bigger than all conflicts) instead of Abstain. If a client receives $|Retries| + |Commit| \geq 3f+1$ (but $|Commit| < 3f+1$) votes, it retries as explained above.

\fs{(If a TX is not in conflict with the same TX at smaller TS, then one technically does not need to wait for the abort to happen before the retry. However, this does allow byz clients to issue multiple committing instances at the same time, and we might want to avoid that).}

Note, that dependents of the original transaction do not carry over to the retry version as they differ in transaction ID. \fs{otherwise retries could violate legality of dependents reads}

\iffalse
Retries for different ID:

Protocol:
Client receives 3f+1 Abstain/Retry votes. If 2f+1 of those are retry (implying that there could be 3f+1 commit votes) a client retries:
1. Aborts the orignial slot on the fast path
2. Commits new version with new timestamp. New ID must be old ID, bar timestamp change. (Other TX are rejected - this avoid multiple TX per client). Do this redefining TxID = (digest, TS). \fs{if we go with this, then change orignial def to this right away.}

Comments:
1. Byz client may just commit same TX twice. That is safe, since Tx needs to pass validation check at 2 times. The higher TX strictly dominates the lower TS. Conflicts are from read.version up to TS.
 

2. Dependent will be aborted. (TS does not match dep read version anymore) This is fine, since the dependent would have been aborted anyways. Is there a way to "save" the dependents too?

\fi












\paragraph{Shard decisions in asynchrony}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fs{How to get from logged decision to shard certificate: We do not implement this optimization}

When network message delays are not withing synchronous bounds an \textit{interested} client may not receive $4f+1$ matching decisions to form a shard-certificate until time-outs grow large enough. To short-circuit this wait, we can add an aditional, non critical-path, roundtrip in order to achieve liveness when tail-latencies are inauspiciousus: 
Concretely, a client may use $3f+1$ matching decisions as proof to issue a $Phase3 \coloneqq (TxID, decision, \{Phase2R_r\}$ decision message. Replicas echo this decision, and clients may return to the Writeback phase with just $3f+1$ $Phase3R$ messages. In order to maintain safety, the \textit{Decision Reconciliation Rule} must be modified. Replicas additionally report existing $Phase3$ decisions to a Fallback replica. If there exist $f+1$ Phase3 decisions we need to adopt this decision \fs{both as p2 and p3 decision is fine}. This is safe because a) no two Phase3 decisions can co-exist in the same view \fs{since 3f+1 votes must intersect in one honest}, b) the existance of a logged decision implies that any $Phase3$ message must comply with this decision, and c) the existance of a $Phase3$ certificate implies that the decision will be logged. \fs{decisions go by view precedence: phase 2 from higher view counts over phase 3 from lower view. These can be inconsistent if the decision was not logged and a p3 is generated, a view change happens without the p3 messages, and now the p3 are different from p2. But those p3 could not have been returned, otherwise f+1 would have been included in view change. Replicas dont send p3 for smaller view once new view is adopted.}
We remark, that this optimization does \textbf{not} imply that a client must wait for three roundtrips to complete, but rather may use the $Phase3R$ decisions if waiting on remaining $Phase2R$ decisions turns out to be slower. Furthermore, in a truly asynchronous network guaranteeing liveness is impossible alltogether, as a (honest) fallback replica might never be elected and consequently reconciliation might never occur.


\paragraph{Personalized Read Leases}: 
Read Timestamps allow executing read transactions to tentatively acquire additional commit confidence by avoiding concurrent write conflicts to manifest. However, this opportunity also empowers byzantine participants that may acquire read timestamps \textit{without} ever intending to commit a transaction. Unlike committing transactions, that are guaranteed to be eventually logged and audited (if it pertains to honest participants interest), there is no point of accountability for such behavior. It is indistinguishable from arbitrarily long executing transactions and would require explicit additional logging efforts. In order to avoid targeted abuse, we re-iterate the rationale of \textit{expecting} timely client performance. Clients that are untimely are, to the system, no better than malicious actors and need to be excluded for performability purposes as the scalability of Indicus is directly related to the industriousness of clients. 
We point out, that a persistant Read Timestamp only implies that write transactions with smaller timestamps may abort. As time moves on, old Read Timestamps become obsolete as new writes no longer conflict. The objective is therefore limit the repetitiveness of such behavior.
Since Read Timestamps are not necessary for safety, they can be granted both as temporary lease and on a per-client basis. Read Timestamps that are evaluated for conflicts past their \textit{Expiration Time} are ignored. This can be implemented in a straightforward fashion by re-defining Read Timestamps $RTS \coloneqq (TS, exp )$, where exp is an expiration time assigned by the replica $exp \coloneqq R.localClock + \Delta_c$. An RTS is only valid during the MVTSO check if $localTime < exp$. The offset $\delta_c$ can be tuned for each client indivdually, based on its past timeliness; Untimely clients may eventually be granted no Read Timestamp, or, be exlcuded from the system.
\fs{ALTERNATIVE: We grant only limited Read Timestamps by redefining $RTS \coloneqq clientTS - \tau_c$, where $\tau_c$ is a defense treshold for each clients reads. The threshold is initialized to zero ($\tau_c \leftarrow 0$), and grows (or shrinks again) based on client untimeliness. Thus, RTS initially grants read protection against all writes with $write.TS < read.TS$, but eventually protects only against $write.TS < read.TS - \tau_c$ (i.e. slow clients are granted smaller protection window against concurrent transactions).}




\paragraph{Hierarchical transaction IDs.}
When transactions span multiple shards, the resulting transaction object includes ReadSet, WriteSet and dependencies from different involved shards. Consequently, every involved shards must validate, and store keys/values from all shards (potentially the whole database) instead of only keys assigned to the respective shard. To circumvent this, we split the transaction object into multiple sub-objects that pertain only to the respective shard, shown in Figure \ref{fig:FigureTxId}. Each sub-object $TxS_j$ has an individual identifier $id_j = H(TxS_j)$ that represents the involved shards of a transaction. A $TxID = H(involvedShards, TS)$ serves as common identifier of the transaction across all shards.

\begin{figure}
\begin{center}
\includegraphics[width= 0.5\textwidth]{./figures/TxIDs.png}
\end{center}
\caption{Logging}
\label{fig:FigureTxId}
\end{figure}

During Validation, client send the entire transaction object to each involved shard \fs{since this will be garbage collected upon Writeback, it is only temporariy more info}. This allows replicas from any shard to aid recovery, in case a malicious client never sends the request to every involved shard. \fs{If we didnt have the full TX, we would be forced to abort sub transactions that have never been sent which would allow byz clients to always abort themselves}
 A replica in shard $S_1$ checks, whether all $TxS_j$ match their digest $id_j$, whether every $TxS_j$ maps to an involved shard, and whether all involved Shards match the $TxID$. It then performs the MVTSO-check only for $TxS_1$. If $TxS_1$ contains a key that does not belong to $S_1$ this serves as PoM \fs{a byz client messed up}. 
For Writeback, it suffices for clients to send only the sub-object $TxS_j$ relevant to shard $S_j$. Since a valid Writeback message must include a shard-decision for each involved shard, it is implied that every shard has durably replicated the Validation transaction object. Thus, it is no longer necessary for shard $S_j$ to keep track of $TxS_i$ ($i \neq j$) for liveness purposes \fs{the other shards contain the info themselves}, and it may store only the objects relevant to its shard. A replica can validate the legitimacy of $TxS_j$ by recomputing $id_j$ and comparing to the signed instance in the shard-decision.\\
 

%-------------------------------------------------------------------------------
\subsection{Indicus3}
%-------------------------------------------------------------------------------
When a replication degree of $n =5f+1$ is deemed too costly, an application can defer to the use of Indicus3. Indicus3 requires the optimal number of replicas $n = 3f+1$ in order to tolerate byzantine faults, at the cost of several design trade-offs.

First, Indicus3 cannot experience a Fast-Path for Committing transactions as it is impossible to log a Commit decision based on the votes alone. Since Indicus3 uses fewer replicas, we adjust Quorums sizes accordingly: any protocol Quorum must contain $2f+1$ replicas (where previously we chose either $4f+1$ or $3f+1$). Only if Commit votes are unanimous can we proceed to log a Commit decision. This is necessary to maintain Isolation guarantees through Quorum intersection (for any two Quorums, they have at least one honest replica in common). Even when $3f+1$ replicas vote to Commit, a later interested Client (or a byzantine client attempting to equivocate) might observe only $f+1$ commit votes since it cannot reliably wait for Quorums of size $\geq n-f$. Thus, there may be no Fast-Path. 

Second, logging a decision requires both an additional round-trip, and certificate overhead in order to guarantee that reconciliation maintains SAF. To convey the intuition behind this, consider the following example where $2f+1$ replicas returned $Phase2R$ decision Commit, and the $f$ remaining replicas are both honest and decided Abort. Any Fallback replica charged with reconciliation must be able to make a decision based on $n-f = 2f+1$ decisions. However, in the presence of byzantine replicas, the observed Quorum might only contain a single Commit decion and $2f$ Abort decisions. No conclusive reconciliation can be made as it is indistinguishable what decision might have been "logged". To break this stalemate, we must introduce an additional phase $Phase3$: A decision is only logged, when $f+1$ honest replicas adopts a $Phase3$ decision, and $2f+1$ matching $Phase3R$ messages are necessary in order to form a shard-certificate. A $Phase3$ message respectively requires the endorsement of $2f+1$ matching $Phase2R$ messages. Thus, by Quorum intersection, it is impossible for two different legal $Phase3$ decisions to co-exist. To maintain SAF, it must moreover be possible to recover a logged decision based on a single $Phase3$ decision. This in turn implies, that decisions must include the $Phase2R$ Quorums as proof to maintain Isolation integrity. The fallback reconciliation rules must be extended accordingly. Note, that a $Phase3$ decision might never exist if $Phase2$ decisions are not consistent. Therefore, the orignial reconciliation rules must be preserved, with precedence given to $Phase3$ decisions, if existant.

These trade-offs are reminiscent of the additional overheads required to offer Retries. In fact, Indicus3 inquires no additional cost to provide Retries as Indicus3's modification satisfy the same functionality.

We point out, that while Indicus3 incurs additional round-trips compared to Indicus5, total latency is not necessarily higher, as Indicus5 relies on larger Quorums and is hence gated by higher-tail latency.
To nevertheless reduce the gracious execution latency we can modify Indicus3 with either one of the  two optimizations options outlined below. Neither of these optimizations are compatible with Retries however:
\textbf{Option A:} We can offer a $Phase2R$ "Fast-Path", by allowing a shard-certificate to be formed on the basis of $3f+1$ matching replies. The existance of such a certificate implies that any $Phase3$ decision must comply. Further, in absence of any $Phase3$ decision, it is possible to reconcile a decision consistently by recovering a decision if $f+1$ Phase2 decisions exists.

\textbf{Option B:} When byzantine participants do not comply or the network is asynchronous, receiving $n =3f+1$ matching replies may be impossible. Nevertheless, we can allow Commit decisions to form shard-certificates within just two roundtrips by denying Abort decisions this possiblity. By introducingaAssymmetry to the protocol, it is possible to safely recover based on a single $Phase2$ Commit decision. This introduces additional cost for Abort decisions as they must complete an additional $Phase3$ in order to complete. \fs{commits will also always get a shard certificate by doing another phase if necessary}. The reconciliation rule must be adjusted accordingly in hierarchical fashion: A single $Phase3$ Abort decision takes precedence over a single $Phase2$ Commit decision, which in turn takes precedence over an arbitrary recovery. This is safe, as a Commit and Abort Shard decision cannot co-exist as they each must have been established on the premise of $2f+1$ $Phase2$ decisions. 
Adding additional cost for Aborts is sensible if the application expects conflicts to be rare. In this case Commit decisions form the common protocol operation, and are optimized accordingly. Note, that the role of Commit and Abort can be interchanged trivially.

Lastly, we want to remark that Indicus3 offers weakened Byzantine Independence guarantees. Concretely, it is possible for a byzantine client to determinsitically abort its own transactions by only relying on vote Quorums that include a colluding byzantine replica. Thus, honest transactions that claim respective dependencies are exposed to the whim of byzantine clients. This can be circumvented only by declining to ever accept dependencies.

