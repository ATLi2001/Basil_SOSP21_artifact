Looking beyond treating transactions as black box requests and leveraging existing commutativity is not a new idea. However, while copious efforts exist to design and build more decentralized systems in order to exploit transaction knowledge for the Crash Failure model, few, if any attempts have been made for the Byzantine Fault Model. This naturally begs the question why so? One explanation is that for BFT systems centralization is in fact highly desirable as it simplifies the problem by pin-pointing a single point of accountability. Since historically, BFT systems have taken a rather niche role for applications that require "additional" safety, the principal design concern has always been maintaining consistency with efficiency and scalability being secondary concerns. Replication was traditionally done by a single authority and thus it is reasonable to assume a primary backup scheme, since neither fairness, nor total ordering were major concerns. As Byzantine Fault Tolerance moves into the mainstream with the popularity ascent of Blockchains these considerations are being revistited. For example, when operating a database as a consortium of mutually distrustful parties it is no longer desirable to grant priviledges to a leader and impose centralization for the sake of simplicity.
Rather than trying to make a traditional BFT system more scalable we ask the question whether we can take the scalability lessons from Crash Failure settings and improve upon their robustness. While this is an attractive avenue, it is far from being straightforward. A natural way to scale a system is to move state and responsabilites to the clients.  
However, in a byzantine setting, giving up the comfort of a bounded and accountable set of replicas opens up the system to a wider set undesirable phenomena. Yet, this is exactly what we will do: Concretely, in this paper we will show how to design a BFT system named Indicus that is almost entirely client driven, and all the while both safe and live. \\

\textbf{Overview:}\\
Unlike State Machine Replication (SMR) based systems that achieve agreement on computation (i.e. state transitions or request results) by imposing a total order for execution, Indicus evaluates results out of order. Reducing the problem of agreement to a sequencing problem simplifies SMR design, yet squanders available commutativity between requests.  Agreement on a total order strengthens the requirment of the system, when potentially unecessary. Concretely, agreement on a totally ordered ledger implies agreement for the entire history prefix for each new request. While desirable in some cases (a fundamental principle of Blockchains), this is unnecessary if any given request is commutative to any other. In fact, in this extreme case, all requests could reach agreement in parralel and out of order. In practice, a partial order, that only orders non-commutative requests suffices.
 To leverage this, Indicus performs agreement for each transaction seperately, and imposing only an implicit order when conflicts arrive. Abstractly, Indicus proposes each transaction for a single-shot binary consensus, i.e. transactions do not compete for shared slots. In order to maintain a serializable Transaction history, Indicus follows a standard Optimistic Concurrency Control technique called Timestamp Ordering (TSO): Indicus pre-defines a suggestion for a total order by assigning a Timestamp to each Transaction. Ordering conflicts between Transactions whose interleavings would violate prescribed Isolation guarantees are broken based on the given Timestamp and speculative execution results.


Overall, the Indicus design has the following positive outcomes:
\begin{itemize}

\item Indicus is robust to censorship and frontrunning as there exist no central authority (in SMR traditionally a leader) that decides what Transactions enter the system and decides on the ordering of Transactions
\item Indicus allows commutative/non-conflicting Transactions to both be executed and validated out of order, thus maximizing parallelism. (This should increase throughput and reduce latency, because clients arent waiting for all previously sequenced tx to finish. Consequently the tail latency does not dominate throughput as much.)
\item Indicus minimizes the state, communication and computation load on replicas, as Clients serve as both execution hub and broadcast channel for their own Transactions. This avoids quadratic communication communication complexity in the normal case. (Theoretically always, but we keep some for practicality in the fallback)
\item As any Quorum system, Indicus is inherently load-balanced as there exists no leader bottleneck and all replicas have equal responsibility
\item In Indicus liveness is a client local property. Unlike SMR, where the entire system halts during view changes, Byzantine participants may stall system progress only for the objects their transactions touch. Hence, the system appears live to any non-conflicting Transaction
\end{itemize}

Since we do not sequence Transaction execution we require a concurrency control mechanism in order to maintain Isolation between Transactions.
For this purpose, we design and implement a byzantine replicated MVTSO scheme, an aggressive version of Optimistic Concurrency control that empowers Read Transactions in the hope of reducing abort rates. Specifically, we allow to read old version, we allow to read uncommitted writes and we acquire read leases.

Additionally, we propose definitions for what it means to enforce an Isolation level in a transactional system with byzantine participants. These are general purpose formalizations and can serve as guideline for future byzantine Database systems.

In section X, we discuss Limitations of this approach. 

