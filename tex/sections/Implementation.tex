%-------------------------------------------------------------------------------
\section{Implementation}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\section{Evaluation}
%-------------------------------------------------------------------------------

Our evaluation seeks to answer the following questions:
\begin{itemize}
  \item How do the throughput and latency of \sys{} compare to the
    state-of-the-art distributed replicated transactional key-value
    stores?
  \item What are the overheads of providing Byzantine fault tolerance over
    crash fault tolerance?
  \item How is the commit rate affected by the choice of concurrency control?
  \item What is the tradeoff between leader-based replication and quorum-based
    replication?
  \item How does \sys{} perform when clients or replicas exhibit Byzantine
    behavior?
\end{itemize}

\paragraph{Baselines.} TAPIR, OCC+PBFT, Hyperledger.
\paragraph{Setup.} Use CloudLab machines, add WAN latency with \texttt{tc}.
\paragraph{Methodology.} Run experiments for several minutes, exclude results
from beginning and end, closed loop clients.

\subsection{Applications}

\begin{figure}
  Scatter Plot\\
  X-axis: throughput (committed new order transactions per second)\\
  Y-axis: latency (median)\\
  Series: Indicus, TAPIR, OCC+PBFT, Hyperledger\\
  Workload: TPC-C\\
  Setup: WAN, 3 shards / groups\\
  \caption{Throughput versus latency of \sys{} and baselines for the TPC-C
  workload in a wide-area network deployment.}
  \label{fig:tpcc-tput-lat}
\end{figure}

We measure the throughput and median latency of \sys{} and the baselines with the
industry standard TPC-C benchmark. Each system is configured to tolerate $f=1$
failures and replicas are deployed across a WAN. Clients are co-located with
replicas in each geographic region, so the round trip time between a client and
its nearest replica is much less than the wide-area network round trip time when
all replicas are correct and responsive. The data is partioned across
three groups of replicas. Figure~\ref{fig:tpcc-tput-lat} shows the results.

TAPIR achieves the lowest median latency when not saturated because reads only
need to be executed at the nearest replica and it often commits transactions in
a single round trip on the fast path of its commit protocol.

Hyperledger, OCC+PBFT, and \sys{} require reads to be
executed at $f+1$ replicas, so each read requires a wide-area network round trip
to the second nearest replica to a client.

Once the execution phase is complete,
\sys{} often commits in a single round trip on the fast path to all $5f+1$
replicas as in TAPIR. Hyperledger and OCC+PBFT take an additional round trip
because their commit protocols are run by a leader replica at each shard.

TAPIR also achieves the highest throughput before saturation because it does not
require expensive signature computations and validation of proofs. \sys{} achieves
better throughput than Hyperledger and OCC+PBFT because no single replica handles
every transaction for a group as is the case with the leader replicas in Hyperledger
and OCC+PBFT.

\subsection{Microbenchmarks}

\begin{figure}
  Bar Graph\\
  Y-axis: throughput at saturation (committed transactions per second)\\
  Workload: YCSB+T, moderate skew\\
  Series: Indicus (no proofs, no signatures), Indicus (proofs, no signatures),
    Indicus (no proofs, signatures), Indicus (full), TAPIR\\
  Setup: WAN, 3 shards / groups\\
  \caption{Throughput of \sys{} with increasing amounts of Byzantine
  ``protection'' for fixed YCSB+T workload.}
  \label{fig:bft-overhead-tput}
\end{figure}

We measure the maximum throughput of \sys{} and TAPIR with the YCSB+T workload at
moderate skew to observe the performance impact of tolerating Byzantine failures.
Figure~\ref{fig:bft-overhead-tput} shows the results.

Indicus with no proofs and no signatures comes within 10\% of the throughput of
TAPIR. This is because Indicus' commit protocol exchanges around the same
number of messages per replica as TAPIR's commit protocol, with
transactions committing after either one round trip to every replica or after
two round trips to a majority of honest replicas.  TAPIR still achieves slightly
better throughput because reads are only executed by a single replica whereas in
Indicus, reads are executed by $f+1$ replicas.

Indicus with signatures and no proofs achieves a lower maximum throughput than
Indicus with no signatures and no proofs because signature computations are
expensive.

Similarly, the computation required to serialize, deserialize, and validate
proofs reduces the throughput of Indicus with no signature and proofs relative
to Indicus with no signatures and no proofs and TAPIR.

Indicus with both signatures and proofs achieves the lowest maximum throughput.
In total, tolerating Byzantine failures imposes about a 75\% throughput overhead
for this workload.

\begin{figure}
  Bar Graph\\
  X-axis: grouped by geographic region\\
  Y-axis: latency at fixed non-saturated load (average + 25th and 75th percentiles)\\
  Workload: YCSB+T, moderate skew\\
  Series: Indicus, TAPIR\\
  Setup: WAN, 3 shards / groups\\
  \caption{Region-by-region latency of \sys{} and TAPIR for fixed YCSB+T workload.}
\end{figure}

Figure~\ref{fig:bft-overhead-lat} shows the latency overheads of tolerating
Byzantine failures. Using the same workload as before, we measure the average,
first quartile, and third quartile latencies. 

\sys{} has higher latency than TAPIR in all geographic regions because its
reads must be executed by $f+1$ replicas instead of just a single replica. This
requires reads to be sent across the wide-area network, which increases total
transaction latency in proportion to the number of reads in the transaction.

\begin{figure}
  Scatter Plot\\
  X-axis: Zipfian parameter\\
  Y-axis: Abort Rate (1 - committed / attempts)\\
  Workload: YCSB+T\\
  Series: Indicus, TAPIR\\
  Setup: WAN, 3 shards / groups\\
  \caption{Abort rate of \sys{} and TAPIR with increasing skew.}
\end{figure}

Figure~\ref{fig:bft-overhead-aborts} shows the commit rate overhead of tolerating
Byzantine failures. It shows the abort rate of transactions from the YCSB+T
workload with increasing contention. 

Because \sys{}'s execution phase takes longer than TAPIR, there is a larger
window during which conflicting transactions can overlap. This directly leads
to more aborts.

\begin{figure}
  Scatter Plot\\
  X-axis: Zipfian parameter\\
  Y-axis: Abort Rate (1 - committed / attempts)\\
  Workload: YCSB+T\\
  Series: Indicus+OCC, Indicus+MVTSO-RTS-DEP, Indicus+MVTSO-DEP, Indicus+MVTSO\\
  Setup: WAN, 3 shards / groups\\
  \caption{Abort rate of \sys{} with different types of concurrency control
  increasing skew.}
\end{figure}

Figure~\ref{fig:cc-aborts} shows how the choice of concurrency control in
\sys{} affects the abort rate of transactions under increasing levels of 
contetion.

Indicus+OCC uses basic OCC validation that disallows concurrenct conflicting
transactions from committing. Indicus+MVTSO improves (reduces the number of
aborts) on this by allowing transactions to read from older versions and by
allowing concurrent conflicting write transactions to commit if their assigned
timestamps match their validation orer at each shard.

Indicus+MVTSO+RTS further improves abort rate by reducing the size of the window
during which transactions execution overlaps. 

Indicus+MVTSO+RTS+DEP improves on this even further by allowing transactions to
read uncommitted writes. This comes at the risk of cascading aborts.

\begin{figure}
  Scatter Plot\\
  X-axis: throughput (committed transactions per second)\\
  Y-axis: latency (median)\\
  Series: Indicus+OCC, OCC+PBFT\\
  Workload: YCSB+T\\
  Setup: WAN, 3 shards / groups\\
  \caption{Throughput versus latency of \sys{} w/ simple OCC check and
  OCC+PBFT in a wide-area network deployment.}
  \label{fig:to-vs-po-tput-lat}
\end{figure}

Figure~\ref{fig:to-vs-po-tput-lat} shows the overhead from the leader-based
OCC+PBFT relative to the quorum-based \sys{}. Latency is higher because leader-based
protocols require redundant wide-area round trips during the validation phase.
Throughput is lower because the leader of each shard processes every transaction
that touches the shard whereas in quorum-based protocols, the processing load 
may be spread equally across all replicas in the shard.

\subsection{Failures}

\begin{figure}
  Scatter Plot\\
  X-axis: time (since start of experiment)\\
  Y-axis: throughput (committed transactions per second)\\
  Series: Indicus\\
  Workload: TPC-C\\
  Setup: WAN, 3 shards / groups, Byzantine replica failure at midway point of
  experiment.\\
\end{figure}

\begin{figure}
  Scatter Plot\\
  X-axis: time (since start of experiment)\\
  Y-axis: throughput (committed transactions per second)\\
  Series: Indicus\\
  Workload: TPC-C\\
  Setup: WAN, 3 shards / groups, Byzantine clients attack system at midway point
  of experiment.\\
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Questions:
\begin{itemize}
  \item Are there any experiments where we explcitily want to measure behavior in LAN?
  \item Hyperledger uses stored procedures/one-shot transactions - how do we compare this to Indicus? Do we use the read-only + conclusory transaction strategy? Or rewrite workloads as one-shot for Hyperledger?
  \item 
\end{itemize}

Question: Is clock skew something we need to explicitly handle? Clock sync etc across machines?

Plan for the evaluation:
1) Questions and experiments that answer those.
  1. compare against state of the art: Tapir, vs Hyperledger (our version)
  

- Need TPC-C
- Try to get Hyperledger to run
- If benefit of partial order is worse than engineering optimization its not worth doing. 
- Could add smart contract layer (like ethereum evm) on top of ours(? probably dont want this)

Is there code that inputs smallbank into hyperledger smart contracts api. ?


2) Which experiments and write the sections on what we expect. What do the graphs look like.


Headline graph: Show performance compared to TApir, hyperledger, occ+pbft.

\subsection{Experiment Notes}
What experiments do we want? Phrase as questions

\begin{itemize}
\item What overhead does a BFT database add compared to a CF database? Show the tradeoff between fault tolerance and performance.
Corresponding experiment: Compare both throughput and latency with Tapir. Also evaluate Abort rate (caused by additional execution time overheads).

Run some (or one) of the Benchmarks: Retwis, YCSB, Smallbank, (TPC-C)?

Show: Throughput vs Latency graph, Skew vs Abort Rate graph

Single data center vs WAN? We probably only want Wan? In low latency environment our validation overheads etc dominate, but our RR reads are not as costly.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item How big is the benefit of avoiding a total order?
Corresponding experiment: Compare against a SMR protocol (PBFT). Single shard evaluation. TX execution the same, but validation in total order. Measure throughput.

- Can do both only validation, or full execution smr (but in this case reads would be local so it is not a fair comparison - also unclear how this case would work multi-sharded).
Do the PBFT validation with both MVTSO and a trivial OCC. Measure only the validation overhead: Since execution should be same? (differs slightly for OCC, no prepared reads)

Do it for multiple shard settings: 2, 4, 6 shards; objects distributed uniformly?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Whats the use of our Concurrency Control techniques
Compare MVTSO vs just TSO and vs OCC
TSO: disable read timestamp check, disable prepared Reads, analyze abort rates. Also need to not do multi version: I.e. Timestamp chosen at the end to be max (current time, max read version)
OCC: 

Tapir compares vs OCC-store (which would be our Pbft equivalent.. Should our PBFT store example just do normal OCC? I.e. pbft for every read request: check if there is uncommitted write -> abort read. Check validation against buffered writes: 
 Was there any read ongoing that is not yet committed

\item How big is the benefit of avoiding redundant coordination?
Corresponding experiment: Multi-sharded setting, show that the "partial" order through sharding is not as good as a true partial order.

--> Run our OCC-store version (pbft validation) with multiple shards and compare. Measure throughput and latency.

\item How does Indicus perform under different workloads? When is it practical and when is it not?
Corresponding experiment: Observe abort rates under contention

Increase throughput and skew. 


\item How does Indicus perform under failures?
Corresponding experiment: Let a fraction of clients time out which results in a fallback invocation. Should show that the throughput hardly gets affected; only tail latency increases.

\item What are the design agnostic overheads?
Corresponding experiment: Measure the costs of signature generation/validation etc. This is cost that is related to CPU power.

\item How does the system behave when scaling number of shards or shard size
Corresponding experiment: Evaluate different shard and replication degrees.

For constant shard size: Increase the number of objects a TX touches (if shard allocation is random this implies 

Expected: More shards increases throughput, but only up to some count, because then coordination overheads increase. More nodes per shard? Should not affect throughput? Latency depends on tail latency - so it can be better or worse depending on how nodes are located. More nodes = bigger proffs = more processing
\item Optional: How does Indicus stack up against a real-world system (albeit with different interfaces etc)?
Corresponding experiment: Compare against Hyperledger throughput. Assume fixed tx sets.
\end{itemize}




