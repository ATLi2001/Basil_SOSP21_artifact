%-------------------------------------------------------------------------------
\section{Implementation and Evaluation}
%-------------------------------------------------------------------------------



\subsection{Experiment Notes}
What experiments do we want? Phrase as questions

\begin{itemize}
\item What overhead does a BFT database add compared to a CF database? Show the tradeoff between fault tolerance and performance.
Corresponding experiment: Compare performance with Tapir
\item How big is the benefit of avoiding a total order?
Corresponding experiment: Compare against a SMR protocol (PBFT). Single shard evaluation. TX execution the same, but validation in total order. Measure throughput.
\item How big is the benefit of avoiding redundant coordination?
Corresponding experiment: Multi-sharded setting, show that the "partial" order through sharding is not as good as a true partial order.
\item How does Indicus perform under different workloads? When is it practical and when is it not?
Corresponding experiment: Observe abort rates under contention
\item How does Indicus perform under failures?
Corresponding experiment: Let a fraction of clients time out which results in a fallback invocation. Should show that the throughput hardly gets affected; only tail latency increases.
\item What are the design agnostic overheads?
Corresponding experiment: Measure the costs of signature generation/validation etc. This is cost that is related to CPU power.
\item Optional: How does Indicus stack up against a real-world system (albeit with different interfaces etc)?
Corresponding experiment: Compare against Hyperledger throughput. Assume fixed tx sets.
\end{itemize}




