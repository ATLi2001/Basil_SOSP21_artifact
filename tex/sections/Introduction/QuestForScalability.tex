\section{On the quest for scalable, decentralized, fault tolerance}
\fs{some intro sentence.} 
In the following we discuss both the limitations of existing solutions and desirable properties that drive Indicus' design. 

\subsection{In light of partial ordering}
\iffalse
Notes:
Talk more about total order. 
And how we really would like to enforce it when necessary only.
How we abstractly treat it as seperate registers on which we can run independent consensus

Databases do it right: They do a partial order and only show the equivalence. 
Hyperledger is a database in its on way: Exec, order, validate architecture. Uses CF consensus however - could be swapped out.\\
\fi


Imposing a shared total order across different machines solves the problem of State Machine Replication, i.e. the objective of maintaining consistent data replicas. It does so however, by a reduction of problem, rather than solving the original goal: providing consistent output. While execution in a common order implies agreement on a common result, this additional step adds coordination overhead, when possibly unecessary, as each transaction is declared to be co-dependent with all other transactions. 
Concretely, when requiring agreement on a totally ordered ledger of transactions, achieving agreement on each new request implies achieving agreement on the entire history prefix. This is desirable in settings relying on probabilistic finality (such as permissionless blockchains), however unecessary in traditional BFT/permissioned blockchain settings. In fact, when all transactions are commutative, all requests could reach agreement in parallel and out of order. Thus, in practice, where only a subset of transactions conflict, a partial order suffices. 
Many large-scale distributed systems consist primarliy of unordered and unrelated operations. For example, a product supply chain consists of many concurrent steps that do not require ordering.
This is the bread and butter of decades of database research. Transactions are executed concurrently and may interleave arbitrarily, while Concurrency Control mechanisms enforce an implicit partial order by resolving conflicts only when necessary. The premise of such an design is simple: Databases allow for a natural, partially ordered execution, that merely maintains the equivalence to a sequential execution.
Existing blockchain technologies \fs{such as Hyperledger Fabric} often consider themselves shared databases, yet fall back to restrictive total order broadcast primitives, hence fundamentally limiting scalability. 


\subsection{A shard of truth}
To combat this scalability bottleneck many systems rely on sharding. Orignially a database technique, sharding partitions the datastore into smaller and faster \textit{shards} that are easier to manage. While sharding approaches in practice \fs{(often)} can induce partial ordering and increase parallelism, this is not a principled property as, in this case, the application of sharding conflates two objectives:
At its core, sharding horizontally scales hardware resources, reducing memory overheads such as storage cost or lookup time \fs{, and general CPU load, such as messages and agreement instances to manage}. In order to maintain a consistenct database, sharding requires coordination across shards, typically in the form of a joint two-phase commit and concurrency control protocol. 
When there exist exploitable spacial locality within the data, i.e. transaction access patterns are limited to a subset of shards coordination across all shards is not necessary and a partial order on transactions may manifest. Note however, that chosing a suitable partition data mapping is highly workload dependent and hence \fs{from a system design perspective} is not agnostic to the application.
Partial order execution and replication is therefore not an inherent property of sharding, but rather a by-product of the desired "share-nothing" partition architecture. \fs{super awkward sentence, replace}

In fact, when transactions span multiple shards, the latent total order requirment within each shard introduces redundant coordination overheads, as coordination may need to be performed twice, at the level of individual shards, and across shards. This is especially problematic when workloads are geo-replicated, or when, as in BFT, the replication factor is high. 
We argue, that there exist no optimal calibration for the degree of sharding: When sharding is coarse (few large shards), scalabiliy suffers under the per-shard total order, wheras when sharding is fine-grained (a lot of small shards), cross-shard overheads begin to dominate. 
Instead of relying on sharding to achieve partial-order transaction processing, it is desirable to build atop primitives that inherently assume a partial order.
Zhang et al (cite ordering paper), where the first to point out, that replication and isolation are not orthogonal problems. By integrating both replication and transaction layer, and consequently relaxing the total order requirement within shards, redundant coordination overheads can be reconciled.
In Indicus we adopt this rationale by co-designing concurrency control and replication, thus naturally exposing existing commutativty between transactions.



\subsection{Decentralizing decentralized systems}
Blockchain technologies are often advertised as a shared database that decentralizes trust (i.e. Hyperledger). While this is upheld, to an extent, in permissionless, open-membership settings (in practice mining pools dominate), this is rarely the case in permissioned, fixed-membership blockchain settings. In order to achieve better performance and scalability, relative to slow and wasteful techniques such as Proof of Work (PoW), the fixed membership bounds allow systems to rely on traditional BFT SMR, such as the seminal PBFT protocol and its descendants, in order to provide the totally ordered log abstraction.
In practice, most of these solutions intertwine the requirement for a total order with the existance of a dedicated leader replica that acts as centralized sequencer. While simple, \fs{(or arguably necessary to establish a live agreement protocol on a totally ordered ledger (i.e. a BFT paxos cannot be live without some exponential backoff and decently? synchronized clocks)} a dedicated leader exposes two obvious concerns: scalability and fairness.

A single sequencer exposes a proposal bottleneck, as load is focused entirely on the leader while other replicas sit idle. Moreover, when the leader is faulty, i.e. crashes or misbehaves, the system comes to a halt, requiring expensive re-election schemes to re-gain liveness while maintaining consistency. \fs{this should only be mentioned superficially here, and concretely only in related work} (BFT-MIR takes a step towards reconciling this bottleneck by allowing all replicas to act as proposers for a subset of the request space (cite BFT MIR). This approach scales the proposal bottleneck by a factor of $n$, but remains coarse grained under failures, as an entire partition of the request space remains unavailable during re-assignment.)

Moreover, a dedicated sequencer exposes a fairness vulnerability. A malicious, or simply rationally biased proposer may censor, delay or frontrun transactions with only limited accountability. Such authority is antithetical to the ethos of decentralized trust and thus highly undesirable. Variations of this scheme enforce either periodic or continuous leader rotation, yet only help to mititage the problem, as each leader has undisclosed authority during its tenure. \fs{probably omit:(Multi-leadered systems, such as BFT-MIR suffer the same problem, as request spaces need to be partitioned in order to avoid request duplication).} \fs{( a hypothetical BFT EPaxos would not have the censorship problem? unsure)} 

These restrictions motivate us to design a leaderless protocol. Concretely, we avoid proposal delegation to an untrusted replica by declaring each client to be its own transaction coordinator.
By shifting responsibility from replicas to clients, the proposal bottleneck becomes the max-capacity of incoming requests replicas can process \fs{(from 1 to n to c "leaders" - need to distinguish better what leaderless means: No replica leader, each client is its own leader, akak coordinator)}, while fairness is entirely up to the network delivey. In doing so, the crux of the design becomes how to tolerate byzantine clients. In particular, byzantine participants should not be able to interfere with honest participants in an unbounded manner and without being held accountable for their participation. \fs{An ideal mechanism would allow honest participants to act fully independently of all byzantine actors. While guaranteeing this is impossible (strong statement), we can give a client the keys to its own liveness. comment: this excludes lack of progress due to concurrency - in this case only exponential backoff can help}



\subsection{A byzantine empire}
Leveraging commutativity between transactions is not a new idea. As a research trend of mitigating the scalability bottleneck induced by total, or redundant ordering requirements, EPaxos (cite), CURP (cite), TAPIR (cite) or Carousel (cite) only consider the ordering between potentially conflicting operations. However, these systems assume the stronger crash-failure model and are non-trivial to extend to the Byzantine model, so that they cannot directly solve the problem of data sharing among mutually distrustful, malicious or arbitrarily failing parties.
\fs{sort of redundant with the last subsection (talking about byz client challenges)}
While copious efforts exist to design decentralized systems that exploit transaction semantics for the crash failure model, few, if any attempts have been made for the Byzantine Fault Model. This naturally raises the question, why so? One explanation is that in Byzantine Systems, some centralization is in fact highly desirable as it simplifies the problem by identifying a single point of accountability. This however, comes at the cost of both scalability and fairness, as outlined in the previous paragraph. \fs{Say something about what makes us believe there is a viable alternative way} Avoiding this leader bottleneck paves the path for a wider set of undesirable phenomena. The challenge of a leaderless byzantine system is simple yet daunting: It empowers both malicious users and system components to collude and misbehave in potentially unaccountable ways. \fs{why do we think this is a good idea? Answer: we dont know, but if it were possible, it would be desirable?}
In Indicus, we will show how to overcome this challenge by limiting a clients immediate control to its own transactions while offering tools to aid termination of transactions requests considered stalled.

\subsection{A transactions only christmas wish}
Most BFT SMR, Blockchains or existing transactional systems (cite callinicos) support transactions under the assumption that their read and write operations are known a priori, limiting the set of applications they can support. In Indicus we assume an interactive transaction model. A Client may execute an arbitrary set of reads and writes, and decide to abort at any time. This is the most general transaction model and offers the most flexibility for applications.

\fs{"A TX can be whatever it wants to be. Its parent DB doesnt prescribe its career."}

Other TX models:\\
- mini transactions \cite{aguilera2007sinfonia}. Read/write keys and values pre-defined\\
- one-shot/stored procedures \cite{mu2016consolidating, mu2014extracting}. Can only touch one partition\\
- 2FI (2 round fixed set interactive) \cite{yan2018carousel}. Keys fixed, write keys fixed, but values may depend\\
Most database systems (54\%) use interactive transaction \cite{pavlo2017we}, only 16\% use one-shot for more than half the workload. (However, this citation also argues that most DBs do not use serializable, but instead read committed - allows for non-repeatable reads, write skew, inconsistent snapshots)

\subsection{Linearity}
\fs{Maybe dont have this section. }\\
Finally, Indicus aims to be as efficient as possible in its agreement protocol. In order to scale with regards to clients \fs{(weird phrasing)}, we avoid unecessary all-to-all communication between replicas and instead utilize clients as broadcasts channels for their own transactions. Moreover, we present two versions of Indicus, Indicus3 and Indicus5, that trade off replication degree for reduced latency and signature overheads respectively. While Indicus3 achieves the optimal minimal replication degree in a BFT system, Indicus5 achieves optimal latency and minimal signature overheads.


\subsection{Indicus' heel}
\fs{perhaps currently redundant with intro}
Indicus' achilles heel is its speculative nature. Like any system with optimistic concurrency control, Indicus is vulnerable to aborts due to congestion. This is aggregated when replicas may be byzantine and hence local reads do not suffice to maintain integrity. However, reading remotely in a wide area network increases the likelihood for transaction interleavings and consequently abort likelihood. In order to mitigate this we develop a distributed design of multi-version timestamp ordering (MVTSO), an aggressive optimistic concurrency control that minmizes abort windows for read requests. A challenge in doing so efficiently is avoiding abuse by byzantine participants. \fs{this is supposed to reference the read timestamps (no real fix), stalling (fallback mechanism to handle) or dependency hogging (we do deps only on f+1 in order to avoid carrying the entire TX)}

\fs{byantine clients can create that congestion if they have the access control. But you expect this to be limited.Replicas can enforce exponential backoff on clients. Clients can only reactively create targeted congestion if they control the network.  (All of this should not go here)}\\



In this paper we show how to complete this Quest, resulting in our design and implementation of Indicus. In the following we define our system model as well as general properties for reasoning about a byzantine database.