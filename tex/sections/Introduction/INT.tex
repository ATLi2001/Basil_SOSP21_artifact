Safe and efficient online data sharing among mutually distrustful
parties offers exciting opportunities for a variety of applications,
including healthcare~\cite{}, financial services~\cite{}, supply chain
management~~\cite{} and more~\cite{}. It is also challenging, as it
requires building a new breed of systems, which can offer to their
users the functionaalities of a distributed database but do so under
much weaker trust assumptions that the database community has
typically considered.

An increasingly popular answer to this challenge are permissioned
blockchains. These systems use a Byzantine fault tolerant (BFT)
replicated state machine protocol\cite{} to produce a totally ordered
log of client transactions, upon which they layer a database
abstraction.  The obvious appeal of this approach is that it can
immediately benefit from, and  build upon, the existing BFT
literature. However, the total order it imposes on transactions is not
only costly to implement and hard to scale, but it is unnecessary for
many of its target applications. For example, unrelated financial
transactions need not be totally ordered, and supply chains, despite
their name, are actually complex networks that generate and process
many logically concurrent transactions.

This mismatch motivates us to explore in this paper a fundamentally
different design point: instead of layering database functionalities on top
of BFT state machine replication, we aim to build \la{the first (?)} a
{\em Byzantine fault tolerant transactional key-value store}.

In principle, this new approach can yield several benefits.

First, it can overcome the scalability bottleneck of a low-level
implementation based on a totally ordered BFT log. Sharded blockchains
can mitigate this bottleneck, but only to a point, as the concurrency
they can achieve is limited by the degree to which transactions can be
executed on individual shards. In contrast, we aim to leverage
databases' ability, honed over decades of research in concurrency
control, to support highly concurrent transaction processing while
retaining, to ease reasoning about correctness, the capacity to
generate executions equivalent to a total order of commmitted
transaction.


Second, by doing away with an explicit ordering service, it can
sidestep the growing concerns~\cite{} about ensuring that blockchains order
transactions fairly. Some protocols~\cite{} leave that ordering to the
whim of a leader; to limit its discretion, other protocol rotate
that rsponsibility, thus reducing, but not eliminating, the skew that can be
introduced by a malicious or self-interested leader. To make things
worse, there is no rigorous, non-anecdotal definition for what
fairness should mean in this context.

Third, it offers the opportunity to eliminate the redundant efforts of
transactional systems built on top state machine replication, which
enforce strong consistency twice, first running replication and then
running atomic commit~\cite{}.

At the same time, a BFT it also presents obvious challenges


\la{Explain the challenge}\\
\la{Point to our ethos as the way we try to address the challenges}\\
\la{briefly introduce the system, unclear in what detail--but not too much}\\
\la{The describe our experience in building and evaluating: we have implemented... We find...}\\
\la{Conclude with the key contributions}

------------------------------------



This paper introduces Indicus, the first leaderless transactional key-value store that is robust in the Byzantine Fault Model. Indicus aims to mitigate the tension between real-world, highly commutative transaction workloads striving for scalability, and the simplicity that totally ordered ledger abstractions such as Blockchains or State Machine Replication offer.

\fs{this maybe sounds too much like a permissioneless blockchain. It needs to be clear that we target permissioned settings which is by definition not quite "trustless", but we "trust" that only a fraction will try to compromise the system.} Specifically, this paper asks the question: how can we enable \textit{mutually distrustful parties} to consistently, reliably share and scalably share data, while minimizing centralization. \\


The ability to share data online offers exciting opportunities; however, increased datasharing also raises the concern of how to \textit{decentralize trust}. In banking, systems like SWIFT (cite) enable financial institutions to quickly and accurately enable cross-institutional transaction clearance, at the cost of placing their trust in the centralized SWIFT network. In manufacturing, online data sharing can improve accountability and auditing amongst the globally distributed supply chain, but there may not be an identifiable source of trust. Consider the supply chain for the latest iPhone: it spans three continents, and hundreds of different contractors \cite{AppleSup} that may neither trust Apple, nor each other, yet must be willing to share and agree on information concerning the construction of the same product. Even sole entities that distribute their dataceneters globally (i.e. Google, Amazon), may not trust their \textit{own} datacenters located in authoritative domains or legislations out of its control.\\


Recognizing this challenge by both the research and industrial communities, much effort has focused on enabling shared computation \fs{sounds like we will talk about secure Multi party computation} between mutually distrustful parties in the context of Byzantine Fault Tolerance (BFT) and Blockchains. 
Systems proposed in the literature of BFT provide the abstraction of a totally ordered request log; the log is agreed upon by the \textit{n} participants in the system, of which at most \textit{f} can misbehave. In the Blockchain world, Bitcoin and Ethereum have become popular distributed computing platforms providing the same log abstraction while aiming for decentralizing trust and open membership \fs{at much lower throughput, more power consumption, no finality}. At the intersection, systems such as Hyperledger Fabric \cite{Hyperledger} or Ethereum Quorum \cite{EthereumQuorum} aim to cater to Blockchain markets while leveraging traditional BFT approaches for scalability. Efforts to incorporate Blockchain/BFT technologies into market ready infrastructures are pervasive \cite{StateFarmQuorum, AutoInventory, StateFarmQuorum2, HyperledgerTelecom, HyperledgerHealth}. \\


At its core, blockchains are functionally a database \fs{Hyperledger itself claims to be a shared database}; they provide a platform for mutually distrustful participants to jointly replicate transactions.
In this paper however, we argue that existing solutions merely shoehorn the desired functionality of transactional applications rather than catering to the principal requirements of a database. 
While applications in stronger failure domains \fs{(Crash Failure)} are built atop transactional databases whose primary design concern is performance, the design of BFT artifacts has been mainly driven by the fault model rather than the desired database functionality, making scalability a secondary optimization concern. 
Instead, we argue, that the practical \fs{sustainable?} way to service applications requiring BFT is to let the functionality drive the design foundation and build dedicated databases that are co-designed with byzantine resilience. \fs{Have a DB that is BFT, rather than BFT that tries to be DB}\fs{people might disagree}
Concretely, we stipulate that an appropriate Database for the Byzantine Fault Model should 1) maintain the \textit{abstraction} of a sequential execution, 2) be scalable, 3) be resitant to censorship and frontrunning and 4) bound the effects of byzantine components, be it replicas or clients.
We outline how existing approaches fall short of satisfying one or more of these requirements, and finally propose a system to fill this gap.

\fs{paragraphs of shortcomings cut here and moved to section 2 in extended form.}
\iffalse
\fs{CUT from HERE}
First, we argue there exists a fundamental mismatch between the implementation of a totally ordered log and the reality of much large-scale distrubted processing. Many large-scale distributed systems consists primarily of unordered and unrelated operations. For example, a product supply chain consists of many concurrent steps that do not require ordering. Imposing an ordering on non-conflicting operations is not only often unecessary, but costly: participants in the shared computation must vote to order operations and serialize request execution accordingly, thus harming both throughput and end-to-end latency. \fs{people can claim that there exist partially ordered solutions -> Dag blockchains, clairvoyant}\\

Second, while there exists work on mitigating this scalability bottleneck through sharding (cite Omniledger, Chainspace, Callinicos), it remains largely an engineering optimization. At its core, the application of sharding conflates two objectives: Horizontal hardware scaling and transactional concurrency \fs{too vague currently}. When a mapping from data to shards is well chosen (note, that this relies on application specific knowledge) partial ordering between objects on different shards manifests as a side-effect of seperating resources \fs{will others agree to this view?}. In fact, when transactions span multiple shards, the latent total order requirement can introduce redundant coordination overheads, as coordination may need to be performed twice, at the level of individual shards, and across shards \cite{zhang2016operation}. This is especially problematic when workloads are geo-replicated (citation?), or when, as in BFT, the replication factor is high \fs{why? be more specific}. \\

Third, achieving a total order often \fs{What if not? OR: Is it even possible without leader?} necessitates a single point of centralization, in order to propose a sequencing order. This is especially undesirable in trust concerned settings as such a sequencer exposes not only a scalability bottleneck but a fairness vulnerability. A sequencer may be biased, frontrun requests or censor others and is inherently antithetical to the need of decentralized, trustless solutions.\\

Lastly, most BFT/Blockchain systems support transactions under the assumption that their read and write operations are known a priori, which limits the set of applications that they can support. \fs{this is sort of random and not in line with the requirements above}


In summary, prior BFT solutions succumb to one or more of the following fallacies, some of which are correlated: They a) impose a restrictive total order, b) use leader in some form or another, c) assume fixed transaction sets or d) incur redundant coordination overheads when sharding. \\

\fs{this paragraph feels like taking a step back}
Leveraging commutativity between transactions is not a new idea. As a research trend of mitigating the scalability bottleneck, EPaxos (cite), TAPIR (cite) and CURP (cite) only consider the ordering between potentially conflicting operations. However, these systems assume the stronger crash-failure model and are non-trivial to extend to the Byzantine model, so that they cannot directly solve the problem of data sharing among mutually distrustful, malicious or arbitrarily failing parties.
\fs{CUT TILL HERE}
\fi

Existing research, in essence, is either attempting to build concurrency control and sharding functionalities over BFT replication, or integrating these functionalities into a crash-failure replication protocol. In this paper, we will show how to build these desiring functionalities inside a BFT replication protocol. Specifically, our goal is to \textit{provide the illusion of a centralized shared log, rather than the non-scalable reality of a totally ordered log}.\\

\fs{paragraph on CF systems cut and moved to section 2}
\iffalse
\fs{rambling too long}
\fs{CUT FROM HERE}

While copious efforts exist to design decentralized systems that exploit transaction semantics for the crash failure model, few, if any attempts have been made for the Byzantine Fault Model. This naturally raises the question, why so? One explanation is that in Byzantine Systems, some centralization is in fact highly desirable as it simplifies the problem by identifying a single point of accountability. A natural way to improve both scalability and fairness is to avoid this bottleneck, at the cost of paving the path for a wider set of undesirable phenomena. The challenge of a leaderless byzantine system is simple yet daunting: It empowers both malicious users and system components to collude and misbehave in potentially unaccountable ways.
In this paper we will show to overcome this challenge by designing and implementing the leaderless BFT system Indicus that avoids near-all centralization while replicating interactive ACID transactions in a partial order. 
\fs{client driven protocol could be seen as nightmare for byz system. What is our design agenda to control this? -> Byz can only influence itself. Honest users can keep using the system }\\
\fs{CUT TILL HERE}
\fi

In this paper we will show to realize this goal by designing and implementing a leaderless BFT database system called Indicus, that avoids near-all centralization while replicating interactive ACID transactions in a partial order. Distinctively, Indicus does not aim to be the 701th BFT State Machine Replication protocol (cite the next 700 bft), but the first BFT protocol to integrate replication and transaction layer.

Indicus' design is driven by the ethos of \textit{independent operability}; both safety \fs{(integrity, serializability)} and liveness \fs{ability to issue and complete TX} are per-client properties and independent from one client to another.
\fs{Indicus embraces partial order across the entire transaction stack} Unlike State Machine Replication (SMR) based systems that achieve agreement on computation (i.e. state transitions) by imposing a total order on execution across all replicas, Indicus executes transactions speculatively at clients, while validating and replicating \textit{all} transactions in arbitrary order at any given replica. Furthermore, clients coordinate the agreement protocol themselves, thus putting them in charge of their own transactions. Doing so, while tolerating byzantine client behavior, is Indicus' main challenge; In Indicus \textit{the client is king - but rulers have to be regulated}.

Abstractly, Indicus proposes each transaction for a seperate concurrent binary consensus instance and enforces an implicit partial order by aborting transactions when inconistencies arise. In order to maintain a serializable transaction history, Indicus assigns each transaction an optimistic timestamp and uses a variation of Multiversioned Timestamp Ordering (MVTSO) for concurrency control. Ordering conflicts between transactions whose interleavings would violate prescribed Isolation guarantees are broken based on the given timestamps and speculative execution results. \fs{does all of this need to be in an intro? There needs to be a little of key insight, but it feels more appropriate for discussion}\\



Overall, the Indicus design has the following implications:
\begin{itemize}%[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]

\item Indicus is robust to censorship and frontrunning as there exist no central authority that admits and orders transactions.
\item Indicus allows commutative/non-conflicting Transactions to both be executed and validated out of order, thus maximizing parallelism. \fs{ (This should increase throughput and reduce latency, because clients arent waiting for all previously sequenced tx to finish. Consequently the tail latency does not dominate throughput as much.)}
\item Indicus minimizes the state, communication and computation load on replicas, as Clients serve as both execution hub and broadcast channel for their own Transactions. \fs{optional:}(This avoids quadratic communication communication complexity in the normal case.) \fs{(Theoretically always, but we keep some for practicality in the fallback)}
\item As any Quorum system, Indicus is inherently load-balanced as there exists no leader bottleneck and all replicas share equal responsibilities.
\item In Indicus liveness is a client local property. Unlike common SMR protocols, where the entire system halts during view changes, Byzantine participants may stall system progress only for the objects their transactions touch. Hence, the system appears live to any non-conflicting Transaction.
\end{itemize}

To achieve these properties Indicus incurs two main tradeoffs. First, shifting responsibility from replicas to clients comes at the cost of higher computational requirements for clients, which may not be tolerable for all applications. In practice we envision Indicus clients to be dedicated transaction managers, rather than end users. Second, like all optimistic concurrency control schemes, Indicus is vulnerable to congestion. When contention is high, abort rate soars; Indicus is not designed for such applications. Exploring tradeoffs between client/replica responsibilities as well as pessimistic concurrency control mechanisms are potential avenues for future work.

Results:  ...

To summarize, this paper makes three main contributions:
\begin{enumerate}
\item It offers definitions for transactional Isolation guarantees for the Byzantine Fault Model
\item It presents the design, implementation, and evaluation of the first leaderless ACID transactional system that tolerates Byzantine Failures
\item It presents two variations (Indicus3 and Indicus5) of a client centric agreement mechanism that trade-off performance for replication degree.  
\item \fs{our MVTSO?}
\end{enumerate}

\fs{perhaps unecessary}
The rest of the paper is structured as follows. In section 2 we elaborate the shortcomings of existing systems and challenges that the proposed design faces. In section 3 we delineate the system model and offer general purpose definitions for reasoning about byzantine databases. In section 4 and 5 we outline the Indicus architecture and protocols respectively. Section 6 describes our evaluation, while section 7 discusses limitations of the Indicus design. Finally, section 7 concludes the paper. 
