


This paper introduces Indicus, the first leaderless transactional key-value store that is robust in the Byzantine Fault Model. Indicus aims to mitigate the tension between real-world, highly commutative transaction workloads striving for scalability, and the simplicity that totally ordered ledger abstractions such as Blockchains or State Machine Replication offer.

\fs{this maybe sounds too much like a permissioneless blockchain} Specifically, this paper asks the question: how can we enable \textit{mutually distrustful parties} to consistently, reliably share and scalably share data, while minimizing centralization. \\


The ability to share data online offers exciting opportunities, however, increased datasharing also raises the concern of how to \textit{decentralize trust}. In banking, systems like SWIFT (cite) enable financial institutions to quickly and accurately enable cross-institutional transaction clearance, at the cost of placing their trust in the centralized SWIFT network. In manufacturing, online data sharing can improve accountability and auditing amongst the globally distributed supply chain, but there may not be an identifiable source of trust. Consider the supply chain for the latest iPhone: it spans three continents, and hundreds of different contractors \cite{AppleSup} that may neither trust Apple, nor each other, yet must be willing to share and agree on information concerning the construction of the same product. Even, sole entities that distribute their dataceneters globally (i.e. Google, Amazon), may not trust their \textit{own} datacenters located in authoritative domains or legislations out of its control.\\


Recognizing this challenge by both the research and industrial communities, much effort has focused on enabling shared computation between mutually distrustful parties in the context of Byzantine Fault Tolerance (BFT) and Blockchains. 
Systems proposed in the literature of BFT provide the abstraction of a totally ordered request log; the log is agreed upon by the \textit{n} participants in the system, of which at most \textit{f} can misbehave. In the Blockchain world, Bitcoin and Ethereum have become popular distributed computing platforms providing the same log abstraction while aiming for decentralizing trust and open membership. At the intersection, systems such as Hyperledger Fabric \cite{Hyperledger} or Ethereum Quorum \cite{EthereumQuorum} aim to cater to Blockchain markets while leveraging traditional BFT approaches for scalability. Efforts to incorporate Blockchain/BFT technologies into market ready infrastructures are pervasive \cite{StateFarmQuorum, AutoInventory, StateFarmQuorum2, HyperledgerTelecom, HyperledgerHealth}. \\



In this paper however, we argue that existing solutions merely shoehorn the desired functionality of transactional applications rather than catering to the principal requirements. While applications in stronger failure domains are built atop transactional Databases whose primary design concern is performance, applications in Byzantine Failure assumptions must resort to systems designed instead for fault tolerance as primary design concern and scalability as optimization concern. \fs{people might disagree}
Concretely, we stipulate that an appropriate Database for the Byzantine Fault Model should 1) maintain the \textit{abstraction} of a sequential execution, 2) be scalable, 3) be resitant to censorship and frontrunning and 4) bound the effects of byzantine components, be it replicas or clients.
We outline how existing approaches fall short of satisfying one or more of these requirements, and finally propose a system to fill this gap.

First, we argue there exists a fundamental mismatch between the implementation of a totally ordered log and the reality of much large-scale distrubted processing. Many large-scale distributed systems consists primarily of unordered and unrelated operations. For example, a product supply chain consists of many concurrent steps that do not require ordering. Imposing an ordering on non-conflicting operations is not only often unecessary, but costly: participants in the shared computation must vote to order operations and serialize request execution accordingly, thus harming both throughput and end-to-end latency. \fs{people can claim that there exist partially ordered solutions -> Dag blockchains, clairvoyant}\\

Second, while there exists work on mitigating this scalability bottleneck through sharding (cite Omniledger, Chainspace, Callinicos), it remains largely an engineering optimization. At its core, the application of sharding conflates two objectives: Horizontal hardware scaling and transactional concurrency \fs{too vague currently}. When a mapping from data to shards is well chosen (note, that this relies on application specific knowledge) partial ordering between objects on different shards manifests as a side-effect of seperating resources \fs{will others agree to this view?}. In fact, when transactions span multiple shards, the latent total order requirement can introduce redundant coordination overheads, as coordination may need to be performed twice, at the level of individual shards, and across shards \cite{zhang2016operation}. This is especially problematic when workloads are geo-replicated (citation?), or when, as in BFT, the replication factor is high \fs{why? be more specific}. \\

Third, achieving a total order often \fs{What if not? OR: Is it even possible without leader?} necessitates a single point of centralization, in order to propose a sequencing order. This is especially undesirable in trust concerned settings as such a sequencer exposes not only a scalability bottleneck but a fairness vulnerability. A sequencer may be biased, frontrun requests or censor others and is inherently antithetical to the need of decentralized, trustless solutions.\\

Lastly, most BFT/Blockchain systems support transactions under the assumption that their read and write operations are known a priori, which limits the set of applications that they can support. \fs{this is sort of random and not in line with the requirements above}


In summary, prior BFT solutions succumb to one or more of the following fallacies, some of which are correlated: They a) impose a restrictive total order, b) use leader in some form or another, c) assume fixed transaction sets or d) incur redundant coordination overheads when sharding. \\

\fs{this paragraph feels like taking a step back}
Leveraging commutativity between transactions is not a new idea. As a research trend of mitigating the scalability bottleneck, EPaxos (cite), TAPIR (cite) and CURP (cite) only consider the ordering between potentially conflicting operations. However, these systems assume the stronger crash-failure model and are non-trivial to extend to the Byzantine model, so that they cannot directly solve the problem of data sharing among mutually distrustful, malicious or arbitrarily failing parties.
Existing research, in essence, is either attempting to build concurrency control and sharding functionalities over BFT replication, or integrating these functionalities into a crash-failure replication protocol. In this paper, we will show how to build these desiring functionalities inside a BFT replication protocol. Specifically, our goal is to \textit{provide the illusion of a centralized shared log, rather than the non-scalable reality of a totally ordered log}.\\

\fs{rambling too long}
While copious efforts exist to design decentralized systems that exploit transaction semantics for the crash failure model, few, if any attempts have been made for the Byzantine Fault Model. This naturally raises the question, why so? One explanation is that in Byzantine Systems, some centralization is in fact highly desirable as it simplifies the problem by identifying a single point of accountability. A natural way to improve both scalability and fairness is to avoid this bottleneck, at the cost of paving the path for a wider set of undesirable phenomena. The challenge of a leaderless byzantine system is simple yet daunting: It empowers both malicious users and system components to collude and misbehave in potentially unaccountable ways.
In this paper we will show to overcome this challenge by designing and implementing the leaderless BFT system Indicus that avoids near-all centralization while replicating interactive ACID transactions in a partial order. 
\fs{client driven protocol could be seen as nightmare for byz system. What is our design agenda to control this? -> Byz can only influence itself. Honest users can keep using the system }\\

The principle of \textit{partial ordering} \fs{doesnt sound good} forms the core of Indicus' design. Unlike State Machine Replication (SMR) based systems that achieve agreement on computation (i.e. state transitions) by imposing a total order on execution across all replicas, Indicus executes transactions speculatively at clients, while validating and replicating \textit{all } transactions in arbitrary order at any given replica. Abstractly, Indicus proposes each transaction for a seperate concurrent binary consensus instance and enforces an implicit partial order by aborting transactions when inconistencies arise. In order to maintain a serializable transaction history, Indicus assigns each transaction an optimistic timestamp and uses a variation of Multiversioned Timestamp Ordering (MVTSO) for concurrency control. Ordering conflicts between transactions whose interleavings would violate prescribed Isolation guarantees are broken based on the given timestamps and speculative execution results. \fs{does all of this need to be in an intro? There needs to be a little of key insight, but it feels more appropriate for discussion}\\

\fs{In Indicus, client is king. However, this entails the responsibility of ruling its kingdom.}


Overall, the Indicus design has the following implications:
\begin{itemize}

\item Indicus is robust to censorship and frontrunning as there exist no central authority that admits and orders transactions
\item Indicus allows commutative/non-conflicting Transactions to both be executed and validated out of order, thus maximizing parallelism. \fs{ (This should increase throughput and reduce latency, because clients arent waiting for all previously sequenced tx to finish. Consequently the tail latency does not dominate throughput as much.)}
\item Indicus minimizes the state, communication and computation load on replicas, as Clients serve as both execution hub and broadcast channel for their own Transactions. This avoids quadratic communication communication complexity in the normal case. \fs{(Theoretically always, but we keep some for practicality in the fallback)}
\item As any Quorum system, Indicus is inherently load-balanced as there exists no leader bottleneck and all replicas have equal responsibilities.
\item In Indicus liveness is a client local property. Unlike SMR, where the entire system halts during view changes, Byzantine participants may stall system progress only for the objects their transactions touch. Hence, the system appears live to any non-conflicting Transaction.
\end{itemize}

To achieve these properties Indicus incurs two main tradeoffs. First, shifting responsibility from replicas to clients comes at the cost of higher computational requirements for clients, which may not be tolerable for all applications. In practice we envision Indicus clients to be dedicated transaction managers, rather than end users. Second, like all optimistic concurrency control schemes, Indicus is vulnerable to congestion. When contention is high, abort rate soars; Indicus is not designed for such applications. Exploring tradeoffs between client/replica responsibilities as well as pessimistic concurrency control mechanisms are potential avenues for future work.

Results:  ...

To summarize, this paper makes three main contributions:
\begin{enumerate}
\item It offers definitions for transactional Isolation guarantees for the Byzantine Fault Model
\item It presents the design, implementation, and evaluation of the first leaderless ACID transactional system that tolerates Byzantine Failures
\item It presents two variations (Indicus3 and Indicus5) of a client centric agreement mechanism that trade-off performance for replication degree.  
\item \fs{our MVTSO?}
\end{enumerate}

