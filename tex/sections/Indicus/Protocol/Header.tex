
%-------------------------------------------------------------------------------
%\section{Protocol}
%-------------------------------------------------------------------------------
\begin{figure*}[!th]
\begin{center}
\includegraphics[width= \textwidth]{./figures/Archi.png}
\end{center}
\caption{{\em Transaction Lifecycle}. Clients execute remote reads (1) and buffer writes (2). For Committment, all involved shards verify isolation (3). If there are conflicting transactions (TX'), replicas in a shard (B) vote to Abort. A client persists a decision (4) that serves as Two-Phase-Commit Vote for each shard (5), and Commits a transaction if all shards vote to commit (6).}
\label{fig:Figure1}
\end{figure*}
\fs{cut figure if it is not useful} \nc{I don't figure hurts, but I don't think it helps much either}\la{I would tentatively remove it: we are probably going to need the space}

\sys is is a sharded and replicated transactional key-value store designed to be scalable and leaderless, and our architecture reflects this ethos. 

\par \textbf{Transaction Execution} Transaction execution is driven by clients (removing costly all-to-all communications amongst replicas) and consists of three phases. First, in an \textit{execution phase}, clients execute individual transactional operations. As is standard in optimistic databases, reads are submitted to remote replicas while writes are buffered locally. \sys{} supports \textit{interactive} and cross-shard transactions: clients can issue new operations based on the results of past operations to any shard in the system. \sys{} must additionally ensure that these read operations do not violate Byzantine independence. In a second \textit{validation phase}, invidual shards in \sys{} must validate whether committing the transaction would violate serializability. For performance, \sys{} allows invidiual replicas within a shard to process requests out of order. \sys{} must additionally ensure that Byzantine actors cannot cause spurious aborts. Finally, \sys{} aggregates each shard decision in a \textit{commit phase} \fs{decision phase? must rename writeback section} to determine the outcome of the transaction, notifies both application and replicas in the system of the decision, and, if the decision was to commit, makes the buffered writes persistent in the data store.  Importantly, the decision of whether each transaction commits or aborts must be preserved across failures, reconfigurations \fs{sounds nice, but we never talk about reconfigs}, and Byzantine attacks. We describe each of these phases in turn in Section~\ref{section:exec}.

\par \textbf{Transaction Recovery} A Byzantine actor could begin executing a transaction, start the validation phase, but intentionally never reveal its decision. Without care,
such behavior would prevent the system from making progress, violating Byzantine independence \fs{I think this refers to independent operability? Byz independence talks about result outcomes}. To ensure progress, \sys{} thus implements a fallback recovery mechanism that can terminate stalled transactions while maintaining \fs{Byz-} serializability. We describe this mechanism in Section~\ref{section:rec}.




\nc{Florian's version in comments}
\iffalse
\sys is designed to be scalable and leaderless. Our architecture reflects this ethos. We briefly summarise it here before going into more detail in the later sections. 
In \sys, clients drive the entire transaction life cycle which can be broken down into three stages as shown in Figure \ref{fig:Figure1}: i) Execution, ii) Validation, and iii) Writeback. 
i) Clients \textit{speculatively execute} transactions themselves, invoking only remote read procedure calls (1) and buffering writes locally (2). \two Clients validate initiate a two-phase commit vote to validate speculative execution results for byzantine-serializability (3).
For every involved shard, a client queries potentially inconsistent replicas for their commitment vote, reconciles divergent votes into a single per-shard decision that maintains Isolation, and make this decision durable to avoid replay of contradictory decisions (4). 
iii) Lastly, clients aggregate all shard decisions (5) for atomic commit, return to the application and asynchronously, \textit{writes back} decisions and database updates (6).

Next, we outline the protocols for Execution, Validation and Writeback respectively. 
\fs{longer version in Architecture.tex }
\fi




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\fs{since this only affect validation, maybe move it there.}
\sys comes in two different flavors, \sys{}3 and \sys{}5 respectively, that rely on varying replication degrees, but implement the same design. \sys{}3 requires $n=3f+1$ replicas \fs{, the minimum bound necessary for BFT SMR?,} per shard to guarantee consistency in the presence of $\leq f$ byzantine replicas. \sys{}5 reduces both latencies during failure free execution and complexity during recovery. \footnote{We believe that consortiums with high performance requirements or high replication degrees are respectively comfortable with paying for additional replicas or tolerating a lower fraction (1/3 vs 1/5) of failures}
For the simplicity of exposition we discuss \sys{}5 for the remainder of the paper, and defer to section X \fs{and/or TR} to describe differences in \sys{}3. In the following, we outline \sys 's execution (unaffected by replication degree), validation and writeback protocols.

\fi



%\input{sections/Indicus/Protocol/subsections/Execution.tex}
%\input{sections/Indicus/Protocol/subsections/ConcurrencyControl.tex}

%\input{sections/Indicus/Protocol/subsections/Validation.tex}

%\input{sections/Indicus/Protocol/subsections/Writeback.tex}
%\input{sections/Indicus/Protocol/subsections/MultiSharding.tex}

%\input{sections/Indicus/Protocol/subsections/Failures.tex}


%\input{sections/Indicus/Protocol/subsections/Optimizations.tex}

 
