\section{Protocol-Notes}

Structure:
0. Preamble what Indicus is and what it is supposed to offer.
1. Model., Properties   
2. Architecture: Exec, Val, WB, 

Replica state?

3. Exec details: Read/Write Set computation
4. Concurrency Control check. Put in relation to Exec details
5. Validation Protocol: a) Voting and Decision rule component b) Logging. Explain similarity to other BFT protocols (logging could use anything, in 5f+1 it resembles Q/U because etc.)
6. Writeback and Multi-shard TX
7. Failures between 5/6
8. Optimizations: 
9. Indicus3: Extra abort phase, different quorums (Validation and View change), Extra proofs
10. Correctness proofs: Only for 5f+1?
	- ACID: serializable specifically
	- Prove recovery maintains same result and consistency
	- Prove liveness (i.e. client has all tools necessary: dependency resolution, general aborts/abstains. Fallback live leader election. )

\subsection{TX execution}
Goal:\\
- clients should be able to read valid and consistent data from the database\\
- clients should see the most recent data\\


Challenge\\
- Replicas can be out of sync and byzantine\\
- There might be concurrent transactions ongoing that are conflicting\\

Our Design:\\
- Clients speculatively execute their TX: Writes buffered locally, Reads go remote\\
- Reads can read committed and potentially committing values\\
- Necessary quorums sized so validity (and liveness) is maintained

Subtlelties:
- unique TX ID -- avoids equivocation and makes Tx uniquely indexable in efficient data structures
- include dependencies in their TX (f+1 signed copies - Q: Why is this necessary? A: To make replicas believe it exists so that other clients are able to recover that knowledge.)
- 

\subsection{Concurrency control}
Goal: \\(Exceptions, read leases, dependencies: Q: how to store full txid)
- limit aborts\\

challenges:\\
- concurrent read/writes. Maintain isolation: serializability\\
- geo-distributed so execution and validation takes long --> more interleavings\\
- clocks are not perfectly synced\\
- minimize unecessary aborts\\
- byzantine clients can create artificial congestion\\

- Maintain Isolation while replicas out of order. In total order protocols (SMR), deviation from the "common" vote signals misbehavior, whereas for us that is not the case. \\

Our Design:\\
- TSO based concurrency control: Assign optimistic timestamps that define serialization order\\
- Enforce loose bound on the timestamps. This is fine for safety, but necessary to restrict progress damage. Alternative way is to have a TS gen phase, but this induces a RTT and certificate to attach so it is undesirable. Instead we ignore Transactions that are too far in the future, as such reads could make a lot of writes abort. Reads are not affected by writes with large timestamps, since we allow to read from RTS time.\\
- Reads and Writes are tested on Conflicts with previously committed or possibly committing Transactions. Evaluation happens based on TS and whether serialization order would be violated.\\
- MVTSO: 3 techniques to reduce the number of aborts:\\
	- Read from TS, and not newest. This allows reads (especially long reads, which can happen in a WAN network) to avoid aborting due to later writes. This requires a multi version store.\\
	- Read possibly committing writes. This is to avoid missing writes that should have been seen. Effectively minimizes the "lockout window" that the latency of validation-writeback phases incur.\\
	- Reads issue RTS in order to acquire "locks" on concurrent writes that would cause reads to abort. This requires the Timestamp to be known in advance. \\
	
Subtlelties:\\
- concurrent read and dependency validation could make own dependencies abort. This can especially be strategically abused by byz clients. To avoid this, add dependency exceptions: Async read exceptions sent by honest clients to minimize this window.\\
- Read leases: Read "locks" could be arbitrarily acquired by byz clients without intention of completing a TX. Thus they are on a timeout. Larger writes are not affected anyways. Application option: decide who to grant read locks, since they are not a safety mechanism, but an additional progress "shield"\\
- Efficient dependency storage: Only claim dependencies on f+1 for several reasons: a) at least one honest believes this could commit, b) allows to only store TXIDS in a TX and not full dependency, because that dep can be recovered when necessary - having info about deps is necessary for client liveness.\\

- how to bound dep depth? Reads return a field that says dep depth. Honest clients will not create deeper deps. Byzantine Clients may try, but cannot, because the f+1 signed dep messages include the full dep tree, and replicas will not accept any TX that have depth > some d. In practice we probably want d = 1, so the fallback starts directly for the immediate depdency.\\
	- in general it was possible to "miss" comittable/prepared tXs, thats okay since reading prepared is just an optimization.\\
	
- Abort decisions need to come with proofs that client can validate: Avoids fake aborts. In 5f+1 the proof is implicit since f+1 abort votes are necessary.\\
- Optimization: Retries for Writes. Comes at a tradeoff for validation phase latency, so we elaborate only later.\\
-
	
Replica datastructures:
- for efficient lookups we have following data structures:
-   Hashmap from TXIDs to protocol state (for easy management of ongoing TX). 
- Key value store for prepared TX, includes Writes, Reads, and RTS
- Committed State: Key value store of the DB. Allows to Read. Allows to lookup for conflicts.
- Commit Log: Set of TXIDS that are committed (maps TXID to TX)  (allows to check if deps have alreaday committed. 
- Abort Log: See Commit Log (allows to check if dependencies have already aborted)
- Dependency set: Map from TXID to waiting dependents (allows to unblock waiting dependencies when a TX finishes)
- (optional: dependant to dependency mapping. Once mapping is empty, this TX is not waiting on anybody anymore)



\subsection{Validation}
Goal:\\
- maintain ACID guarantees: Isolation, Durability, Atomicity\\
- Be client driven, i.e. leaderless and out of order\\
- avoid frontrunning\\
- be scalable\\
- be fast\\


Challenges:\\
- Maintain Isolation while creating highest chance to commit\\
- Tolerate byzantine replicas voting dishonestly\\
- Tolerate Client failures: equivocation, crashes, stalling, replay\\
--> Answer: make final decisions idempotent\\
- enable consistent recovery\\
- minimize state at replicas while still guaranteeing correct agreement. Make sure local knowledge is enough to guarantee global correctness\\
- minimize necessary roundtrips and communication complexity\\


Basic design:\\
- Voting phase:\\
	- Client sends to all, All to CC check, all reply\\
	- Client makes decision based off Quorum and Decision Rule (have figure for it?)\\
	- Metaphor: Instead of a leader serializing and deciding on a decision, the decisin was made jointly by all replicas. Hence, "voting" phase
	
- Decision rule:\\
	- Since votes can be inconsistent (no total order enforced on replicas) we must aggregate them\\
	- Must be designed in a way that maintains Isolation\\
	- 3f+1 commit necessary. Vice versa, 3f+1 abstain necessary. Absense of enough commit votes requires pessimistic decision in abort.\\
	- Wait up to time out for at least 2f+1. If timeout expires and less than 2f+1, keep waiting. \\
	
- Ordering decision is made without a leader (democratically): Fairness is up to the network\\
	
- Persistant Logging. 2pc analogy. (or is 3pc analogy better?)\\
	- Want to make sure, that whatever client decision was made (equivalent to choice of Vote quorum), is going to be persistent and idempotent. I.e. if commit/abort is returned to the application any re-issue of the protocol (which can be necessary under faults) is consistent with the original decision.\\
	- In 2pc decision is logged to persistant storage. That obviously doesnt work in this setting (byz client, even under crash you would like to make progress). So a Client "logs" the decision by replicating it consistently to the replicas.\\
	- Client sends decision, gets back echo\\
	- If enough consistent echos then the result may return. This guarantees that decision can be recovered.\\
	- 4f+1 matching replies required: We will show recovery later.\\
	
- Any "normal" agreement protocol could be used once a "decision" has been cast, at this point it is just for replication. I.e. one could plug in a leader based consensus mechanism here, but its overkill and unecessary. There are no conflicts anymore, so an order isnt necessary either. Just unordered broadcast/receive is enough: This is exactly Q/U basically. "Contention" is a byzantine client equivocating. Q/U makes the observation that with multiple clients it would not be live, thus we introduce the fallback view change mechanism. \\

Subtlelties:\\
- 5f+1 commit fast path: 3f+1 abstain/abort fast path. (Cannot have these with retries). If Abort includes proof, then 1 abort fast path as well.\\
- 


\subsection{Writeback}
Goal: \\
- Finalize commit/aborts\\
- Maintain consistency\\
- Enable garbage collection\\

Challenges:\\
- client failures (crash, equivocation)\\
- 

Basic Design:\\
- Client uses the Logging certificates to proceed. Aggregates these certificates for every shard.\\
- Any Client can fulfill this role, arbitrary amount in parallel or even Replicas because this operation is guaranteed to be idempotent (due to the validation and recovery logic)\\
- Upon Receiving Commit Abort, Replicas update their data structures. Remove from Prepared strucutres, Add to Commit/Abort Logs. \
- Dont need a dedicated append only ledger (?). It would be different for each replica anyways. Can just store these logs as Hashmaps (i.e. reuse our lookup structures)\\
- 

Subtlelties:\\
- Optimization: Single shard logging\\
- The logging phase is redundant if there are multiple voting shards involved. Instead, the votes could be aggregated BEFORE logging to form the decison and then only be logged on a dedicated shard. This saves communication bandwidth and makes recovery simpler, because there is a dedicated single shard where to look.\\

\subsection{Optimizations}
Goals:\\
- reduce write aborts\\
- reduce redundancy in validation\\
- reduce read lock impact (byz clients)\\

Challenges:\\
- defend against byz client abuse\\
- respect all shard results\\
- 

- Pretty much mentioned in other sections already. Seems like its more suitable to have it with the contet right away.?

\subsection{Failures}
Goals:\\
- be robust to byzantine client\\


challenges:\\
- byzantine clients can stall or equivocate during Validation/Writeback\\
- replicas can diverge due to byzantine clients. Need to reconcile safely\\
- Granting honest clients liveness --> ability to "realiably" finish all Transactions.\\


Design: \\
- Since the system overall has no shared notion of progress, we maintain liveness only on a per client bases. Specifically, view changing is not only unecessary if nobody cares about a TX but also useless, as no "liveness" is maintained.\\
- View change seperately for every single TX, because every single TX is consensus on a binary register\\
- Since liveness is directly coupled to Clients, view changes should naturally be coupled to clients interest. However, if multiple Clients are able to run the protocol at the same time, then there might never be a conclusion. Electing a single client to be in charge would be very difficult and also not bound the view changes necessary for progress, because there is an unbounded fraction of byz clients\\
- Solution: Delegate view change to a dedicated fixed size group.. The replicas! Gives the guarantee, that when the network is synchronous, after at most f view changes it will succeed. Sync needs to be assumed and thats ok according to FLP.\\
- A little more subtle: After at most f view changes IF an honest client is involved, otherwise no guarantee\\

LIVE ELECTION\\
- Protocol (in theory): Clients issue View change requests for a TX they are interested in. Replicas ONLY move to the next view if a client proves to them, that a Quorum existed in the past view. This guarantees, that replicas cannot diverge arbitrily far in their views. More concretely, there is always >=f+1 honest replicas that dont diverge in views by more than 1 and there are < f+1 honest replicas that diverge from other replicas by more than 1.\\


- Protocol (in practice): To avoid byzantine Clients view changing arbitrarily long and driving up timeouts before honest Clients are interested we introduce some all to all forwarding.
Add additional (off critical viewchange path) exchange to the Fallback replica to issue and disseminate certificates. (Technically equivalent to a Writeback if single sharded. If multi-sharded 2 considerations: a) if not optimized single shard logging, then still need to aggregate decisions, b) if single shard logging, then replica needs to be aware of all other shards in order to forward (can still Writeback locally).\\

SAFE RECOVERY:\\
- Protocol: Choose 2f+1 if existing, otherwise f+1, otherwise redo p2 based on p1. (p1 guaranteed to exist since they voted. Replicas only vote if they had the p1. Client gave it to them if they didnt have it before.) (If replica sees 4f+1 matching right away, it can return that to the client and also distribute as certs)\\
- Clients can only use Quorums from matching views to return to the application (simple counterexample: 3 commits 3 aborts and then swap)\\
- FB can use decisions from multiple views for the decision rule (i.e. decision rule is view agnostic). When could this arise? Because newer views subsume older views and FBs might have been byzantine, or been replaced too quickly. \\
- Proof by Induction: If something returned then ... guaranteed to only ever issue that decision\\
- 



Necessary Extras:\\
- A Client first asks all replicas for existing p2 state or potential certificates. An honest client doesnt need the view change in that case - it was in order to get such certificates.\\
- Client acquires current view from all replicas by doing so. Uses this to start a view change. If f+1 matching exist, client uses those to catch up replicas that lag behind proactively. (just speeds things up: dont need to send all 4f+1)\\
- A client simultaneously re-issues a p1 message, in case some replicas have never seen the TX\\
- A client is repsonsible for the Writeback (any client can do this): Any "interested" client receives the p2 decisions made from a fallback and attempts to return. \\


Subtletlties:\\
- (Maybe useless:) Only a client that provides a dep proof (f+1) or abstain proof is allowed to issue fallback. Doesnt make it impossible to start FB, but at least some hurdle (a client could always just receive such a abstain message with the proof from a byz colluder replica\\
- Fallback election is not started if that TX itself has a dependency - wait for all deps to be finished themselves. This is to distinguish whether a Client has been slow, or its blocking itself due to another dependency.\\
- Tx map to different initial fallbacks: I.e. (TXID + view) mod n \\


\subsection{Garbage Collection}
Challenge:
- memory footprint grows large due to multi version
- certificates impose large signature overheads

Options:
- Need to bound and remove versions below watermarks
- In 5f+1 can read based on f+1 matching committed. Dont need certs and can still be safe for recovery and robust to replicas "claiming random dependency aborts"


\subsection{Discussion: }
- Explain/Define Client relative liveness. We dont provide liveness if something is async, "liveness" ill defined for this sort of system. What it means: Clients that follow P experience liveness, but there is no system notion because there is no shared total ledger.  Progress is client relative\\
- Several seperate binary consensus instances \\


optional Extras to mention:
-Can use Witnesses instead of replicas to lighten replication burden
- only need to contain key/version and not values


\subsection{Indicus3}



View changes:
- bounding rule works differently: slightly weaker guarantee
- Decision rule: In View change hierarchy!! I.e. higher view beats anything. Implication: If conflicting decision to previous view was made, then previous view vote could not have been final.
	1. 1 p3 commit: use it to return
	2. 1 p3 abort: re-issue p2 abort 
	3. 1 p2 commit: re-issue p2 commit
	4. 1 p2 abort: re-issue p2 abort OR re-do p1, either works
	5. Nothing: re-issue p2 using p1 decisions.
	
Want the same practical all to all between replicas + fallback async certificates 