%\input{sections/Indicus/Protocol/subsections/Execution.tex}
%\input{sections/Indicus/Protocol/subsections/Validation.tex}
%\input{sections/Indicus/Protocol/subsections/ConcurrencyControl.tex}
%\input{sections/Indicus/Protocol/subsections/ConsistentLogging.tex}
%\input{sections/Indicus/Protocol/subsections/Writeback.tex}
%\input{sections/Indicus/Protocol/subsections/MultiSharding.tex}
%\input{sections/Indicus/Protocol/subsections/Failures.tex}
%\input{sections/Indicus/Protocol/subsections/Variations.tex}

%-------------------------------------------------------------------------------
\section{Protocol}
%-------------------------------------------------------------------------------
Indicus comes in two different flavors, Indicus3 and Indicus5 respectively, relying on varying replication degrees. Indicus3 requires 3f+1 replicas per shard to guarantee consistency, the minimum bound necessary for BFT SMR. Indicus5 uses a higher replication degree of 5f+1 replicas, but in return brings down both gracious execution latency and complexity during uncivil executions. We argue, that unlike past system settings where a single authority strives to maintain the fewest amount of replicas necessary for cost considerations, consortium systems with naturally higher replication degree are willing to pay the additional price for performance. Moreover, rather than trying to reach the threshold of replicas to tolerate some number of faults, these settings may start with a fixed number of replicas and consider the ratio of faults to be tolerable. When argueing from this perspective, the perceived difference between tolerating 1/3 and 1/5 of replica failures may be negligible.
For the simplicity of exposition we outline Indicus5 in detail below. In section X we describe the differences in Indicus3.

%-------------------------------------------------------------------------------
\subsection{TEsting}
%-------------------------------------------------------------------------------
\begin{figure}[t]
  \begin{mdframed}[roundcorner=10pt]
 	\textbf{TX}
 	\begin{itemize}
 	\item ClientID
 	\item ClientSeqNo
 	\item ReadSet = \{(key, version)\}
 	\item WriteSet = \{(key, value)\}
 	\item dependencies = \{$\langle (key, version)_{f+1 \sigma} \rangle$\}
 	\item TXID = H(TX)
 	\item Timestamp = (Time, ClientID)  optional:, TXID) this is just deterministic tie breaker when a client misbehaves.
 	\end{itemize}
  \end{mdframed}
  \caption{Transaction}
  \label{fig:TX}
\end{figure}


NEED FIg with replica state. Need to use it to explain what "Prepared"/Committable means
%-------------------------------------------------------------------------------
\subsection{Execution}
%-------------------------------------------------------------------------------
Clients in Indicus both execute and submit their own Transactions. As previously defined a Transaction TX is a sequence of read and write requests that is ultimately terminated by a Commit or Abort decision. A TX object, as shown in Figure \ref{fig:TX} records the execution state necessary for Validation.

Client execution conducts as follows:\\

\begin{enumerate}
\item \textbf{Write(key, value)}. A Client executes a request Write(key, value) by locally buffering (key, value) and returning. Concretely: $WriteSet = WriteSet \cup (key, value)$
\item \textbf{Read(key, TS, RQS)} Given hyperparameter Read Quorum Size (RQS), a Client performs a read on given key at timestamp TS by reading from RQS different replicas. \\

A replica returns a pair $\langle \textit{(Committed, Prepared)} \rangle _{\sigma_r}$, where \\
$Committed \coloneqq (value, version)_{cert}$ such that $ (value, version) \in R.CommitLog$, $version = max(q) : (key, val, q) \in R.CommitLog \land v < TS \}$ and $cert$ is a certificate proving (value, version) was allowed to commit (see Section Writeback) and\\
 $Prepared \coloneqq (value, version)$ such that $(value, version) \in R.PreparedSet$ and $version = max(q) : (key, val, q) \in R.PreparedSet \land Committed < v < TS \}$ .
I.e. Committed is the largest committed write and Prepared is the largest uncommitted write that may be  committed and would make Committed outdated. \\

1. Reading from 1 Replica: could abort
2. Reading f+1 matching Prepared that are larger than the commit --> choose that and add dependency
Guaranteed to see 1 thats not intentionally stale. But still arbitrarily stale if unlucky.
3. If read from 2f+1 then its effectively a read lock. Note, could still have missed a prepared write and have to abort because we required f+1 matching. Why f+1? so it comes from 1 honest: we dont need proofs, and we know it is in the system for recovery.
Still no guarantee that newest commit was seen, but some protection from other writes.


(Note: the TS used here is not the final one, its just the tuple of time and clientId. (The triple later is just necessary to differentiate two TX by a client that were assigned the same Time
Add Read set decision rule, put in correlation to RQS: \fs{could only accept reads if f+1 matching always, then no proofs necessary, but then reads can fail}
Add exceptions etc.
)


\item \textbf{Commit} A Client finalizes its execution, computes the final Timestamp and submits 

\item \textbf{Abort}

\end{enumerate}
 


- Read proofs required for honest client correctness. Alternatively one can read from f+1 only, but that can result in failed/older reads
- Do not need to include read proofs in the prepare. According to Isolation definition byzantine Clients can read whatever they want. Moreover, Reads only have very limited external effect. The value does not matter for the CC check. The version has bounded effect: If it goes towards 0, then it is just a check between timestamps as normally. If it goes towards the TS, then it will never abort.
- Fallback not just useful for dependency cleanup, but also "unclaimable" dependencies that need to finish.
%-------------------------------------------------------------------------------
\subsection{Validation}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Concurrency Control}
%-------------------------------------------------------------------------------
\begin{algorithm}
\caption{MVTSO-Check(TX, TS)}\label{euclid}
\begin{algorithmic}[1]
\If{\textit{$TS > localClock + \delta$} }
\State pass
\EndIf

\For{\textit{$\forall key,version \in \textit{TX.read-set}$}}
        \If{$ \exists TX2 \in CommitLog: key \in \textit{TX2.write-set} \land version < \textit{TX2.TS} < TS$}  
          \State  \Return ABORT, \textit{TX2, TX2.CommitCert}
        \EndIf
         \If{$ \exists TX2 \in Prepared: key \in \textit{TX2.write-set} \land version < \textit{TX2.TS} < TS$}   
          \State  \Return ABSTAIN, \textit{TX2}
        \EndIf
        
   %     \If{$ \textit{dep[key]} == null \land version \notin prepared-reads[key] \cup  CommitLog \cup AbortLog $}   
    %      \State  \Return $\textit{Proof-of-Misbehavior}$
    %    \EndIf
        
        
\EndFor

\For{\textit{$\forall key \in \textit{TX.write-set}$}}
        \If{$\exists TX2 \in Prepared/CommitLog \land TX \notin TX2.dep: \textit{TX2.read-set[key].version} < TS < TX2.TS$}
          \State  \Return RETRY, %$\textit{TS_{new} | \forall TS_{new} > TX2.TS}$ // 
        \EndIf
        \If{$\exists RTS \in key.RTS: RTS > TS \land TX \notin RTS.dep$}
          \State  \Return RETRY, %$\textit{TS_{new} | \forall TS_{new} > TX2.TS}$ // 
        \EndIf

\EndFor
\State Prepared.add(TX) 
\While{$\exists d \in dep: d \notin CommitLog \cup AbortLog $)}
\State Wait
\EndWhile

\For{\textit{$\forall d \in dep$}}
        \If{$ d \in CommitLog $}
        	\If{$d.TS > TS$}
        	\State \Return ABORT, d.CommitCert
          	\EndIf
       
		\Else 
		\State \Return ABORT, d.AbortCert
		\EndIf
\EndFor


\State \Return COMMIT


\end{algorithmic}
\end{algorithm}


 - Optimization: retries - heights
 - Dependency resolution tree
 		- Equivocation not possible if TXidentifier a function with dep as argument
 		- Cannot claim dep if not f+1 times (If you want to, you would require proofs again, which we try to avoid because dep trees can grow exponentially). More reads also more likely to commit --> f+1 guarantees 1 honest thinks it is legit.
 - Exception for depedency and early async read response to inform of exceptions
 - Read leases instead of unlimited locks - in practice only grant to timely clients (not a safety measure, but an increased progress guarantee)
 - 
%-------------------------------------------------------------------------------
\subsection{Consistent logging.}
%-------------------------------------------------------------------------------
\begin{figure}
\begin{center}
\includegraphics[width= 0.4\textwidth]{./figures/AB.png}
\end{center}
\caption{Atomic Broadcast}
\label{fig:Figure1}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width= 0.4\textwidth]{./figures/5f+1.png}
\end{center}
\caption{Logging}
\label{fig:Figure1}
\end{figure}

Principles and challenges

protocol overview: pic


%-------------------------------------------------------------------------------
\subsection{Writeback and Multi-shard 2pc}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Failures}
%-------------------------------------------------------------------------------
- Fallback: election (only starts if not waiting on another dep to avoid early eviction), views, resolution, subtelties with mvtso (block because of dep), necessity even without dependencies. Interested clients, write-back multishard. garbage collection
- Fallback requires an extra round in order to learn about current views to start viewchange, but thats ok: Its co-function with learning about full TX, and checking for existing certificates. Timeout invocation is concurrent with p1 message.

%-------------------------------------------------------------------------------
\subsection{Optimizations}
%-------------------------------------------------------------------------------
- Retries
- single shard logging
\begin{figure*}
\begin{center}
\includegraphics[width= \textwidth]{./figures/SingleShard.png}
\end{center}
\caption{Single Shard Optimization}
\label{fig:Figure1}
\end{figure*}


- (Read Locks ; remove can be optimization for writes)
- OCC instead of mvtso structures if disallowing prepared writes to be visible. OCC if not worried about reads aborting

%-------------------------------------------------------------------------------
\subsection{Garbage Collection}
%-------------------------------------------------------------------------------
- Prepares get removed upon commit/abort. Eventually replica might need to become "interested" client.

%-------------------------------------------------------------------------------
\subsection{Indicus3}
%-------------------------------------------------------------------------------
3f+1 if not defending against byz colluders as much

- no fast path
- Commits in 2 rounds, Aborts in 3 (Alternatively symmetric version)
- fallback quorums and bounds
- proofs necessary for recovery
- recovery rules

\begin{figure}
\begin{center}
\includegraphics[width= 0.4\textwidth]{./figures/3f+1.png}
\end{center}
\caption{Logging}
\label{fig:Figure1}
\end{figure}


